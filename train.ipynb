{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import shutil\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RND = 0\n",
    "RUN = 'C'\n",
    "OUT_DIR = 'out_1m/'\n",
    "TRAIN_TMP_DIR = OUT_DIR + '/train'\n",
    "INPUT_DIR = '/d2/caches/tf-speech/train/audio'\n",
    "TENSORBOARD_DIR = '/tensorboard/tf-speech/%s' % RUN\n",
    "MODELS_DIR = '%s/models/%s' % (OUT_DIR, RUN)\n",
    "INPUT_SIZE = (64, 64, 1)  # n_mels x width x 1ch\n",
    "MSG_NORM_MEAN = 116.536\n",
    "MSG_NORM_STD = 21.5913\n",
    "LABELS = [\n",
    "    'yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go',\n",
    "    'unknown', 'silence'\n",
    "]\n",
    "\n",
    "N_VAL_SAMPLES = 3000\n",
    "N_TRAIN_SAMPLES = 1000000  # how many training samples to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 'lib.ipynb'\n",
    "%run 'data-generator.ipynb'\n",
    "%run 'models.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove tensorboard data\n",
    "if os.path.isdir(TENSORBOARD_DIR): shutil.rmtree(TENSORBOARD_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init data gen\n",
    "dg = DataGenerator(input_dir=INPUT_DIR, labels=LABELS)\n",
    "dg.n_mels = INPUT_SIZE[0]\n",
    "dg.msg_w = INPUT_SIZE[1]\n",
    "# normalization params\n",
    "dg.samplewise_norm = True\n",
    "dg.msg_std = MSG_NORM_STD\n",
    "dg.msg_mean = MSG_NORM_MEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val samples: 3000\n"
     ]
    }
   ],
   "source": [
    "# generate/load val data\n",
    "\n",
    "val_files_path = OUT_DIR + '/val_files.npy'\n",
    "val_X_path = OUT_DIR + '/val_X.npy'\n",
    "val_Y_path = OUT_DIR + '/val_Y.npy'\n",
    "\n",
    "if not os.path.isfile(val_files_path):\n",
    "    # generate, save\n",
    "    dg.val_files = {}\n",
    "    val_X, val_Y = dg.generate_val_set(n=N_VAL_SAMPLES)\n",
    "    np.save(val_files_path, dg.val_files)\n",
    "    np.save(val_X_path, val_X)\n",
    "    np.save(val_Y_path, val_Y)\n",
    "else:\n",
    "    # load\n",
    "    dg.val_files = np.load(val_files_path)\n",
    "    val_X = np.load(val_X_path)\n",
    "    val_Y = np.load(val_Y_path)\n",
    "\n",
    "assert len(val_X) == len(val_Y)\n",
    "print('val samples: %d' % len(val_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training samples: 1000000\n"
     ]
    }
   ],
   "source": [
    "# generate/load training data\n",
    "\n",
    "train_X_file = '%s/train_X.mem' % OUT_DIR\n",
    "train_Y_file = '%s/train_Y.mem' % OUT_DIR\n",
    "\n",
    "if not os.path.isfile(train_X_file):\n",
    "    dg.generate_train_set(\n",
    "        n_total=N_TRAIN_SAMPLES,\n",
    "        n_per_job=1000,\n",
    "        n_pools=16,\n",
    "        X_file=train_X_file,\n",
    "        Y_file=train_Y_file,\n",
    "        tmp_dir=TRAIN_TMP_DIR)\n",
    "\n",
    "train_X = np.memmap(\n",
    "    train_X_file, np.float32, 'r', shape=(N_TRAIN_SAMPLES, ) + INPUT_SIZE)\n",
    "train_Y = np.memmap(\n",
    "    train_Y_file, np.float32, 'r', shape=(N_TRAIN_SAMPLES, len(dg.labels)))\n",
    "\n",
    "assert len(train_X) == len(train_Y)\n",
    "print('training samples: %d' % len(train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Model_2(input_size=INPUT_SIZE, output_size=len(LABELS))\n",
    "model.build()\n",
    "optimizer = RMSprop(lr=1e-3)\n",
    "model.m.compile(\n",
    "    optimizer=optimizer, loss=categorical_crossentropy, metrics=['accuracy']\\\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create models dir\n",
    "if os.path.isdir(MODELS_DIR): shutil.rmtree(MODELS_DIR)\n",
    "os.makedirs(MODELS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LR schedule\n",
    "\n",
    "# def step_decay(epoch, initial_lr=0.0001, drop=.5, epochs_drop=5.):\n",
    "#     lrate = initial_lr * math.pow(drop, math.floor((1 + epoch) / epochs_drop))\n",
    "#     return lrate\n",
    "\n",
    "# plt.plot([step_decay(x) for x in range(0, 25)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples per epoch: 100000\n",
      "\n",
      "Epoch 1/100\n",
      "200/200 [==============================] - 15s - loss: 1.8136 - acc: 0.3872 - val_loss: 0.5292 - val_acc: 0.8177\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 14s - loss: 0.8139 - acc: 0.7360 - val_loss: 0.3901 - val_acc: 0.8777\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 14s - loss: 0.5783 - acc: 0.8162 - val_loss: 0.2824 - val_acc: 0.9173\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 14s - loss: 0.3793 - acc: 0.8781 - val_loss: 0.3492 - val_acc: 0.9163\n",
      "Epoch 5/100\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.1702 - acc: 0.9467\n",
      "Epoch 00004: reducing learning rate to 0.00010000000474974513.\n",
      "200/200 [==============================] - 14s - loss: 0.1706 - acc: 0.9466 - val_loss: 0.3731 - val_acc: 0.9247\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 14s - loss: 0.5678 - acc: 0.8335 - val_loss: 0.2479 - val_acc: 0.9343\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 15s - loss: 0.3865 - acc: 0.8803 - val_loss: 0.2633 - val_acc: 0.9320\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 14s - loss: 0.3824 - acc: 0.8858 - val_loss: 0.2363 - val_acc: 0.9360\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 14s - loss: 0.4556 - acc: 0.8608 - val_loss: 0.2384 - val_acc: 0.9390\n",
      "Epoch 10/100\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.2658 - acc: 0.9175\n",
      "Epoch 00009: reducing learning rate to 1.0000000474974514e-05.\n",
      "200/200 [==============================] - 15s - loss: 0.2655 - acc: 0.9176 - val_loss: 0.2776 - val_acc: 0.9413\n",
      "Epoch 11/100\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4520 - acc: 0.8679\n",
      "Epoch 00010: reducing learning rate to 1.0000000656873453e-06.\n",
      "200/200 [==============================] - 14s - loss: 0.4518 - acc: 0.8679 - val_loss: 0.2559 - val_acc: 0.9410\n",
      "Epoch 12/100\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4056 - acc: 0.8763\n",
      "Epoch 00011: reducing learning rate to 1.0000001111620805e-07.\n",
      "200/200 [==============================] - 14s - loss: 0.4057 - acc: 0.8763 - val_loss: 0.2556 - val_acc: 0.9417\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 14s - loss: 0.2690 - acc: 0.9207 - val_loss: 0.2559 - val_acc: 0.9417\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 14s - loss: 0.0964 - acc: 0.9752 - val_loss: 0.2568 - val_acc: 0.9417\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 14s - loss: 0.0944 - acc: 0.9757 - val_loss: 0.2577 - val_acc: 0.9417\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 15s - loss: 0.2715 - acc: 0.9150 - val_loss: 0.2579 - val_acc: 0.9417\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 14s - loss: 0.2929 - acc: 0.9067 - val_loss: 0.2579 - val_acc: 0.9417\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 15s - loss: 0.2607 - acc: 0.9176 - val_loss: 0.2579 - val_acc: 0.9420\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 14s - loss: 0.1967 - acc: 0.9399 - val_loss: 0.2578 - val_acc: 0.9423\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 15s - loss: 0.1946 - acc: 0.9407 - val_loss: 0.2577 - val_acc: 0.9423\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 14s - loss: 0.4052 - acc: 0.8774 - val_loss: 0.2576 - val_acc: 0.9423\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 14s - loss: 0.4061 - acc: 0.8773 - val_loss: 0.2575 - val_acc: 0.9423\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 15s - loss: 0.2674 - acc: 0.9210 - val_loss: 0.2578 - val_acc: 0.9420\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 14s - loss: 0.0916 - acc: 0.9764 - val_loss: 0.2587 - val_acc: 0.9423\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 14s - loss: 0.0902 - acc: 0.9766 - val_loss: 0.2597 - val_acc: 0.9420\n",
      "Epoch 00024: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd4d0d979b0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "\n",
    "N_PER_BATCH = 500\n",
    "STEPS_PER_EPOCH = len(\n",
    "    train_X) // N_PER_BATCH // 10  # last number splits train set into # epochs\n",
    "N_EPOCHS = 100\n",
    "\n",
    "print('# samples per epoch: %d\\n' % (STEPS_PER_EPOCH * N_PER_BATCH))\n",
    "\n",
    "\n",
    "def train_generator(n_per_batch):\n",
    "    start_i = 0\n",
    "    while True:\n",
    "        if start_i >= len(train_X): start_i = 0\n",
    "        batch_X = train_X[start_i:start_i + n_per_batch]\n",
    "        batch_Y = train_Y[start_i:start_i + n_per_batch]\n",
    "        yield (batch_X, batch_Y)\n",
    "        start_i += n_per_batch\n",
    "\n",
    "\n",
    "model.m.fit_generator(\n",
    "    train_generator(N_PER_BATCH),\n",
    "    STEPS_PER_EPOCH,\n",
    "    epochs=N_EPOCHS,\n",
    "    validation_data=(val_X, val_Y),\n",
    "    callbacks=[\n",
    "        TensorBoard(log_dir=TENSORBOARD_DIR),\n",
    "        ModelCheckpoint(\n",
    "            MODELS_DIR +\n",
    "            '/e{epoch:03d}-l={loss:.5f}-vl={val_loss:.5f}-a={acc:.5f}-va={val_acc:.5f}.h5',\n",
    "            monitor='val_acc',\n",
    "            verbose=0,\n",
    "            save_best_only=False,\n",
    "            save_weights_only=False,\n",
    "            mode='auto'),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.1,\n",
    "            patience=1,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1),\n",
    "        EarlyStopping(\n",
    "            monitor='val_acc',\n",
    "            min_delta=0.00001,\n",
    "            patience=5,\n",
    "            verbose=1,\n",
    "            mode='auto')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
