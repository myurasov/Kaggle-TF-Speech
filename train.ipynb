{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import shutil\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = '0'\n",
    "RND = 0\n",
    "RUN = 'D3'\n",
    "OUT_DIR = 'out_1m/'\n",
    "TRAIN_TMP_DIR = OUT_DIR + '/train'\n",
    "INPUT_DIR = '/d2/caches/tf-speech/train/audio'\n",
    "TENSORBOARD_DIR = '/tensorboard/tf-speech/%s' % RUN\n",
    "MODELS_DIR = '%s/models/%s' % (OUT_DIR, RUN)\n",
    "INPUT_SIZE = (64, 64, 1)  # n_mels x width x 1ch\n",
    "\n",
    "N_VAL_SAMPLES = 3000\n",
    "N_TRAIN_SAMPLES = 1000000  # how many training samples to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make only specific GPU to be utilized\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 'lib.ipynb'\n",
    "%run 'data-generator.ipynb'\n",
    "%run 'models.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove tensorboard data\n",
    "if os.path.isdir(TENSORBOARD_DIR): shutil.rmtree(TENSORBOARD_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val samples: 3000\n"
     ]
    }
   ],
   "source": [
    "# load val data\n",
    "\n",
    "val_X_path = OUT_DIR + '/val_X.npy'\n",
    "val_Y_path = OUT_DIR + '/val_Y.npy'\n",
    "\n",
    "val_X = np.load(val_X_path)\n",
    "val_Y = np.load(val_Y_path)\n",
    "\n",
    "assert len(val_X) == len(val_Y)\n",
    "print('val samples: %d' % len(val_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training samples: 1000000\n"
     ]
    }
   ],
   "source": [
    "# load training data\n",
    "\n",
    "train_X_file = '%s/train_X.mem' % OUT_DIR\n",
    "train_Y_file = '%s/train_Y.mem' % OUT_DIR\n",
    "\n",
    "train_X = np.memmap(\n",
    "    train_X_file, np.float32, 'r', shape=(N_TRAIN_SAMPLES, ) + INPUT_SIZE)\n",
    "train_Y = np.memmap(\n",
    "    train_Y_file, np.float32, 'r', shape=(N_TRAIN_SAMPLES, len(LABELS)))\n",
    "\n",
    "assert len(train_X) == len(train_Y)\n",
    "print('training samples: %d' % len(train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Model_2(input_size=INPUT_SIZE, output_size=len(LABELS))\n",
    "model.build()\n",
    "optimizer = RMSprop(lr=1e-3)\n",
    "model.m.compile(\n",
    "    optimizer=optimizer, loss=categorical_crossentropy, metrics=['accuracy']\\\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create models dir\n",
    "if os.path.isdir(MODELS_DIR): shutil.rmtree(MODELS_DIR)\n",
    "os.makedirs(MODELS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples per epoch: 100000\n",
      "\n",
      "Epoch 1/100\n",
      "200/200 [==============================] - 15s - loss: 2.1088 - acc: 0.2789 - val_loss: 1.0974 - val_acc: 0.6630\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 14s - loss: 1.4686 - acc: 0.5098 - val_loss: 0.7354 - val_acc: 0.7613\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 15s - loss: 1.2078 - acc: 0.6032 - val_loss: 0.5760 - val_acc: 0.8117\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 15s - loss: 1.0490 - acc: 0.6581 - val_loss: 0.5012 - val_acc: 0.8330\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 15s - loss: 0.8959 - acc: 0.7088 - val_loss: 0.4552 - val_acc: 0.8523\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 15s - loss: 0.8698 - acc: 0.7188 - val_loss: 0.3924 - val_acc: 0.8780\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 15s - loss: 0.7446 - acc: 0.7587 - val_loss: 0.3647 - val_acc: 0.8857\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 15s - loss: 0.6931 - acc: 0.7777 - val_loss: 0.3668 - val_acc: 0.8853\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 15s - loss: 0.6830 - acc: 0.7809 - val_loss: 0.3215 - val_acc: 0.9000\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 15s - loss: 0.5553 - acc: 0.8206 - val_loss: 0.3044 - val_acc: 0.9067\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 15s - loss: 0.6451 - acc: 0.7946 - val_loss: 0.2769 - val_acc: 0.9150\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 15s - loss: 0.5066 - acc: 0.8381 - val_loss: 0.2874 - val_acc: 0.9090\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 15s - loss: 0.5132 - acc: 0.8372 - val_loss: 0.2785 - val_acc: 0.9177\n",
      "Epoch 14/100\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.5131 - acc: 0.8377\n",
      "Epoch 00013: reducing learning rate to 1.9999999494757503e-05.\n",
      "200/200 [==============================] - 15s - loss: 0.5127 - acc: 0.8379 - val_loss: 0.2955 - val_acc: 0.9183\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 15s - loss: 0.3638 - acc: 0.8863 - val_loss: 0.2730 - val_acc: 0.9273\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 15s - loss: 0.5125 - acc: 0.8380 - val_loss: 0.2501 - val_acc: 0.9273\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 15s - loss: 0.4617 - acc: 0.8521 - val_loss: 0.2498 - val_acc: 0.9257\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 15s - loss: 0.4409 - acc: 0.8597 - val_loss: 0.2476 - val_acc: 0.9277\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 15s - loss: 0.4212 - acc: 0.8667 - val_loss: 0.2512 - val_acc: 0.9257\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 15s - loss: 0.3642 - acc: 0.8845 - val_loss: 0.2616 - val_acc: 0.9267\n",
      "Epoch 21/100\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.3552 - acc: 0.8870\n",
      "Epoch 00020: reducing learning rate to 3.999999898951501e-06.\n",
      "200/200 [==============================] - 15s - loss: 0.3552 - acc: 0.8870 - val_loss: 0.2656 - val_acc: 0.9280\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 15s - loss: 0.2999 - acc: 0.9045 - val_loss: 0.2614 - val_acc: 0.9267\n",
      "Epoch 23/100\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.3362 - acc: 0.8931\n",
      "Epoch 00022: reducing learning rate to 7.999999979801942e-07.\n",
      "200/200 [==============================] - 15s - loss: 0.3365 - acc: 0.8930 - val_loss: 0.2600 - val_acc: 0.9267\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 15s - loss: 0.3762 - acc: 0.8808 - val_loss: 0.2580 - val_acc: 0.9270\n",
      "Epoch 25/100\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.3711 - acc: 0.8836\n",
      "Epoch 00024: reducing learning rate to 1.600000018697756e-07.\n",
      "200/200 [==============================] - 15s - loss: 0.3709 - acc: 0.8836 - val_loss: 0.2575 - val_acc: 0.9283\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 15s - loss: 0.4379 - acc: 0.8608 - val_loss: 0.2562 - val_acc: 0.9280\n",
      "Epoch 27/100\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4410 - acc: 0.8599\n",
      "Epoch 00026: reducing learning rate to 3.199999980552093e-08.\n",
      "200/200 [==============================] - 15s - loss: 0.4406 - acc: 0.8600 - val_loss: 0.2552 - val_acc: 0.9280\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 15s - loss: 0.4175 - acc: 0.8674 - val_loss: 0.2550 - val_acc: 0.9280\n",
      "Epoch 29/100\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.3746 - acc: 0.8815\n",
      "Epoch 00028: reducing learning rate to 1e-08.\n",
      "200/200 [==============================] - 15s - loss: 0.3746 - acc: 0.8815 - val_loss: 0.2548 - val_acc: 0.9280\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 15s - loss: 0.3751 - acc: 0.8804 - val_loss: 0.2547 - val_acc: 0.9280\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 15s - loss: 0.2981 - acc: 0.9068 - val_loss: 0.2547 - val_acc: 0.9280\n",
      "Epoch 00030: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9feacce6a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "\n",
    "N_PER_BATCH = 500\n",
    "STEPS_PER_EPOCH = len(\n",
    "    train_X) // N_PER_BATCH // 10  # last number splits train set into # epochs\n",
    "N_EPOCHS = 100\n",
    "\n",
    "print('# samples per epoch: %d\\n' % (STEPS_PER_EPOCH * N_PER_BATCH))\n",
    "\n",
    "\n",
    "def train_generator(n_per_batch):\n",
    "    start_i = 0\n",
    "    while True:\n",
    "        if start_i >= len(train_X): start_i = 0\n",
    "        batch_X = train_X[start_i:start_i + n_per_batch]\n",
    "        batch_Y = train_Y[start_i:start_i + n_per_batch]\n",
    "        yield (batch_X, batch_Y)\n",
    "        start_i += n_per_batch\n",
    "\n",
    "\n",
    "model.m.fit_generator(\n",
    "    train_generator(N_PER_BATCH),\n",
    "    STEPS_PER_EPOCH,\n",
    "    epochs=N_EPOCHS,\n",
    "    validation_data=(val_X, val_Y),\n",
    "    callbacks=[\n",
    "        TensorBoard(log_dir=TENSORBOARD_DIR),\n",
    "        ModelCheckpoint(\n",
    "            MODELS_DIR +\n",
    "            '/e{epoch:03d}-l={loss:.5f}-vl={val_loss:.5f}-a={acc:.5f}-va={val_acc:.5f}.h5',\n",
    "            monitor='val_acc',\n",
    "            verbose=0,\n",
    "            save_best_only=False,\n",
    "            save_weights_only=False,\n",
    "            mode='auto'),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=2,\n",
    "            min_lr=1e-8,\n",
    "            verbose=1),\n",
    "        EarlyStopping(\n",
    "            monitor='val_acc',\n",
    "            min_delta=0.00001,\n",
    "            patience=5,\n",
    "            verbose=1,\n",
    "            mode='auto')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
