{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import shutil\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = '0'\n",
    "RND = 1\n",
    "RUN = 'B'\n",
    "OUT_DIR = 'out'\n",
    "TENSORBOARD_DIR = '/tensorboard/tf-speech-v2/%s_$fold$' % RUN\n",
    "MODELS_DIR = '%s/models/run_%s/fold_$fold$' % (OUT_DIR, RUN)\n",
    "INPUT_SIZE = (64, 64, 1)  # n_mels x width x 1ch\n",
    "FOLDS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '../data-generator.ipynb'\n",
    "%run '../models.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_batch(n, train_X, train_Y, train_files, val_files):\n",
    "    assert isinstance(val_files, set)\n",
    "\n",
    "    # extra random indexes to search for files not in val_files\n",
    "    def _extra_indexes():\n",
    "        return np.random.randint(0, len(train_X), size=int(n * 0.15))\n",
    "\n",
    "    ii = np.random.randint(0, len(train_X), size=n)\n",
    "    extra_ii = []\n",
    "\n",
    "    replaced = 0\n",
    "\n",
    "    # replace indexes with files occuring in val_files\n",
    "    for j in range(len(ii)):\n",
    "        if '(silence)' != train_files[ii[j]]:\n",
    "            while train_files[ii[j]] in val_files:\n",
    "                if len(extra_ii) == 0: extra_ii = _extra_indexes()\n",
    "                ii[j], extra_ii = extra_ii[0], extra_ii[1:]\n",
    "                replaced += 1\n",
    "\n",
    "    X = train_X[ii]\n",
    "    Y = train_Y[ii]\n",
    "    files = train_files[ii]\n",
    "    \n",
    "    return X, Y, files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_X): 1000000\n"
     ]
    }
   ],
   "source": [
    "train_X = np.memmap('%s/train_X.mem' % OUT_DIR, np.float32,\n",
    "                    'r').reshape((-1, ) + INPUT_SIZE)\n",
    "train_Y = np.memmap('%s/train_Y.mem' % OUT_DIR, np.float32, 'r').reshape(\n",
    "    (-1, len(LABELS)))\n",
    "\n",
    "train_files = np.load('%s/train_files.npy' % OUT_DIR)\n",
    "\n",
    "assert len(train_Y) == len(train_X)\n",
    "assert len(train_files) == len(train_X)\n",
    "\n",
    "print('len(train_X):', len(train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "N_PER_BATCH = 1000\n",
    "# last number splits train set into XX epochs\n",
    "STEPS_PER_EPOCH = len(train_X) // N_PER_BATCH // 10\n",
    "N_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = np.memmap('%s/test/test_X.mem' % (OUT_DIR), np.float32,\n",
    "                   'r').reshape((-1, ) + INPUT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0\n",
      "len(val_X): 2441\n",
      "models_dir: out/models/run_B/fold_0\n",
      "tensorboard_dir: /tensorboard/tf-speech-v2/B_0\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 49s - loss: 2.5755 - acc: 0.1014 - val_loss: 2.4481 - val_acc: 0.0995\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 47s - loss: 2.1545 - acc: 0.2677 - val_loss: 1.4055 - val_acc: 0.5166\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 47s - loss: 1.3632 - acc: 0.5514 - val_loss: 0.5919 - val_acc: 0.7948\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 47s - loss: 0.8723 - acc: 0.7187 - val_loss: 0.3570 - val_acc: 0.8964\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 47s - loss: 0.6290 - acc: 0.8024 - val_loss: 0.3106 - val_acc: 0.9099\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 47s - loss: 0.4882 - acc: 0.8466 - val_loss: 0.2332 - val_acc: 0.9263\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 47s - loss: 0.3860 - acc: 0.8792 - val_loss: 0.2221 - val_acc: 0.9398\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 47s - loss: 0.3006 - acc: 0.9063 - val_loss: 0.3814 - val_acc: 0.8837\n",
      "Epoch 9/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2357 - acc: 0.9274\n",
      "Epoch 00008: reducing learning rate to 0.00020000000949949026.\n",
      "100/100 [==============================] - 47s - loss: 0.2348 - acc: 0.9277 - val_loss: 0.2318 - val_acc: 0.9422\n",
      "Epoch 10/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9715\n",
      "Epoch 00009: reducing learning rate to 4.0000001899898055e-05.\n",
      "100/100 [==============================] - 47s - loss: 0.0955 - acc: 0.9715 - val_loss: 0.2458 - val_acc: 0.9463\n",
      "Epoch 11/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9821\n",
      "Epoch 00010: reducing learning rate to 8.000000525498762e-06.\n",
      "100/100 [==============================] - 47s - loss: 0.0613 - acc: 0.9821 - val_loss: 0.2549 - val_acc: 0.9476\n",
      "Epoch 12/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9841\n",
      "Epoch 00011: reducing learning rate to 1.6000001778593287e-06.\n",
      "100/100 [==============================] - 47s - loss: 0.0537 - acc: 0.9842 - val_loss: 0.2534 - val_acc: 0.9488\n",
      "Epoch 13/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9850\n",
      "Epoch 00012: reducing learning rate to 3.200000264769187e-07.\n",
      "100/100 [==============================] - 47s - loss: 0.0517 - acc: 0.9850 - val_loss: 0.2542 - val_acc: 0.9484\n",
      "Epoch 00012: early stopping\n",
      "evaluation on holdout:\n",
      "['loss', 'acc']\n",
      "[0.28956849448918365, 0.95099999999999996]\n",
      "158538/158538 [==============================] - 25s    \n",
      "\n",
      "fold: 1\n",
      "len(val_X): 2441\n",
      "models_dir: out/models/run_B/fold_1\n",
      "tensorboard_dir: /tensorboard/tf-speech-v2/B_1\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 47s - loss: 2.5803 - acc: 0.1343 - val_loss: 2.1232 - val_acc: 0.3113\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 47s - loss: 1.8527 - acc: 0.3799 - val_loss: 1.5318 - val_acc: 0.4523\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 47s - loss: 1.2235 - acc: 0.5996 - val_loss: 1.0528 - val_acc: 0.6788\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 47s - loss: 0.8404 - acc: 0.7282 - val_loss: 0.4823 - val_acc: 0.8656\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 47s - loss: 0.1955 - acc: 0.9399 - val_loss: 0.3011 - val_acc: 0.9320\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 47s - loss: 0.1468 - acc: 0.9550 - val_loss: 0.3613 - val_acc: 0.9263\n",
      "Epoch 10/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1081 - acc: 0.9670\n",
      "Epoch 00009: reducing learning rate to 4.0000001899898055e-05.\n",
      "100/100 [==============================] - 47s - loss: 0.1083 - acc: 0.9669 - val_loss: 0.3768 - val_acc: 0.9246\n",
      "Epoch 11/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9760\n",
      "Epoch 00010: reducing learning rate to 8.000000525498762e-06.\n",
      "100/100 [==============================] - 47s - loss: 0.0790 - acc: 0.9760 - val_loss: 0.3729 - val_acc: 0.9312\n",
      "Epoch 12/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9775\n",
      "Epoch 00011: reducing learning rate to 1.6000001778593287e-06.\n",
      "100/100 [==============================] - 47s - loss: 0.0734 - acc: 0.9776 - val_loss: 0.3727 - val_acc: 0.9316\n",
      "Epoch 13/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9794\n",
      "Epoch 00012: reducing learning rate to 3.200000264769187e-07.\n",
      "100/100 [==============================] - 47s - loss: 0.0689 - acc: 0.9794 - val_loss: 0.3744 - val_acc: 0.9312\n",
      "Epoch 14/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9782\n",
      "Epoch 00013: reducing learning rate to 6.400000529538374e-08.\n",
      "100/100 [==============================] - 47s - loss: 0.0719 - acc: 0.9783 - val_loss: 0.3745 - val_acc: 0.9312\n",
      "Epoch 00013: early stopping\n",
      "evaluation on holdout:\n",
      "['loss', 'acc']\n",
      "[0.26705910498129376, 0.94774999999999998]\n",
      "158538/158538 [==============================] - 24s    \n",
      "\n",
      "fold: 2\n",
      "len(val_X): 2441\n",
      "models_dir: out/models/run_B/fold_2\n",
      "tensorboard_dir: /tensorboard/tf-speech-v2/B_2\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 47s - loss: 2.6259 - acc: 0.1408 - val_loss: 1.8704 - val_acc: 0.3240\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 47s - loss: 1.8167 - acc: 0.3823 - val_loss: 1.0020 - val_acc: 0.6460\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 47s - loss: 1.0826 - acc: 0.6443 - val_loss: 0.4803 - val_acc: 0.8468\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 47s - loss: 0.7490 - acc: 0.7590 - val_loss: 0.6072 - val_acc: 0.8296\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 47s - loss: 0.5508 - acc: 0.8275 - val_loss: 0.3207 - val_acc: 0.9025\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 47s - loss: 0.4358 - acc: 0.8641 - val_loss: 0.3182 - val_acc: 0.9127\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 47s - loss: 0.3289 - acc: 0.8977 - val_loss: 0.2988 - val_acc: 0.9304\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 47s - loss: 0.2614 - acc: 0.9190 - val_loss: 0.2588 - val_acc: 0.9385\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 47s - loss: 0.2079 - acc: 0.9356 - val_loss: 0.2891 - val_acc: 0.9324\n",
      "Epoch 10/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1814 - acc: 0.9450\n",
      "Epoch 00009: reducing learning rate to 0.00020000000949949026.\n",
      "100/100 [==============================] - 47s - loss: 0.1810 - acc: 0.9452 - val_loss: 0.2645 - val_acc: 0.9332\n",
      "Epoch 11/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9830\n",
      "Epoch 00010: reducing learning rate to 4.0000001899898055e-05.\n",
      "100/100 [==============================] - 47s - loss: 0.0568 - acc: 0.9830 - val_loss: 0.3211 - val_acc: 0.9426\n",
      "Epoch 12/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9903\n",
      "Epoch 00011: reducing learning rate to 8.000000525498762e-06.\n",
      "100/100 [==============================] - 47s - loss: 0.0334 - acc: 0.9903 - val_loss: 0.3129 - val_acc: 0.9463\n",
      "Epoch 13/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9917\n",
      "Epoch 00012: reducing learning rate to 1.6000001778593287e-06.\n",
      "100/100 [==============================] - 46s - loss: 0.0274 - acc: 0.9917 - val_loss: 0.3180 - val_acc: 0.9472\n",
      "Epoch 14/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9922\n",
      "Epoch 00013: reducing learning rate to 3.200000264769187e-07.\n",
      "100/100 [==============================] - 47s - loss: 0.0261 - acc: 0.9922 - val_loss: 0.3186 - val_acc: 0.9472\n",
      "Epoch 00013: early stopping\n",
      "evaluation on holdout:\n",
      "['loss', 'acc']\n",
      "[0.31244535386000405, 0.95274999999999999]\n",
      "158538/158538 [==============================] - 24s    \n",
      "\n",
      "fold: 3\n",
      "len(val_X): 2440\n",
      "models_dir: out/models/run_B/fold_3\n",
      "tensorboard_dir: /tensorboard/tf-speech-v2/B_3\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 47s - loss: 2.5988 - acc: 0.1190 - val_loss: 2.4909 - val_acc: 0.0832\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 47s - loss: 1.9436 - acc: 0.3373 - val_loss: 1.1348 - val_acc: 0.6246\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 47s - loss: 1.1741 - acc: 0.6119 - val_loss: 0.5498 - val_acc: 0.8139\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 47s - loss: 0.7943 - acc: 0.7437 - val_loss: 0.3367 - val_acc: 0.8943\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 47s - loss: 0.5900 - acc: 0.8126 - val_loss: 0.3153 - val_acc: 0.9164\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 47s - loss: 0.4522 - acc: 0.8578 - val_loss: 0.2723 - val_acc: 0.9283\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 47s - loss: 0.3558 - acc: 0.8900 - val_loss: 0.2713 - val_acc: 0.9316\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 47s - loss: 0.2873 - acc: 0.9118 - val_loss: 0.2900 - val_acc: 0.9328\n",
      "Epoch 9/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2198 - acc: 0.9328\n",
      "Epoch 00008: reducing learning rate to 0.00020000000949949026.\n",
      "100/100 [==============================] - 47s - loss: 0.2198 - acc: 0.9327 - val_loss: 0.2864 - val_acc: 0.9373\n",
      "Epoch 10/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9839\n",
      "Epoch 00010: reducing learning rate to 8.000000525498762e-06.\n",
      "100/100 [==============================] - 47s - loss: 0.0544 - acc: 0.9839 - val_loss: 0.3350 - val_acc: 0.9439\n",
      "Epoch 12/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9867\n",
      "Epoch 00011: reducing learning rate to 1.6000001778593287e-06.\n",
      "100/100 [==============================] - 47s - loss: 0.0432 - acc: 0.9867 - val_loss: 0.3356 - val_acc: 0.9447\n",
      "Epoch 13/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9866\n",
      "Epoch 00012: reducing learning rate to 3.200000264769187e-07.\n",
      "100/100 [==============================] - 47s - loss: 0.0449 - acc: 0.9866 - val_loss: 0.3360 - val_acc: 0.9447\n",
      "Epoch 00012: early stopping\n",
      "evaluation on holdout:\n",
      "['loss', 'acc']\n",
      "[0.30860064729166697, 0.94925000000000004]\n",
      "158538/158538 [==============================] - 24s    \n",
      "\n",
      "fold: 4\n",
      "len(val_X): 2440\n",
      "models_dir: out/models/run_B/fold_4\n",
      "tensorboard_dir: /tensorboard/tf-speech-v2/B_4\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 47s - loss: 2.6000 - acc: 0.1021 - val_loss: 2.4435 - val_acc: 0.0881\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 47s - loss: 2.0692 - acc: 0.3001 - val_loss: 1.6780 - val_acc: 0.4439\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 47s - loss: 1.2729 - acc: 0.5770 - val_loss: 0.6860 - val_acc: 0.7828\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 47s - loss: 0.8645 - acc: 0.7194 - val_loss: 0.3784 - val_acc: 0.8889\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 47s - loss: 0.6358 - acc: 0.7978 - val_loss: 0.2397 - val_acc: 0.9258\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 47s - loss: 0.4935 - acc: 0.8453 - val_loss: 0.2766 - val_acc: 0.9189\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 47s - loss: 0.3827 - acc: 0.8801 - val_loss: 0.2064 - val_acc: 0.9369\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 47s - loss: 0.2913 - acc: 0.9085 - val_loss: 0.2383 - val_acc: 0.9348\n",
      "Epoch 9/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2371 - acc: 0.9267\n",
      "Epoch 00008: reducing learning rate to 0.00020000000949949026.\n",
      "100/100 [==============================] - 47s - loss: 0.2368 - acc: 0.9268 - val_loss: 0.2635 - val_acc: 0.9344\n",
      "Epoch 10/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9715\n",
      "Epoch 00009: reducing learning rate to 4.0000001899898055e-05.\n",
      "100/100 [==============================] - 47s - loss: 0.0935 - acc: 0.9717 - val_loss: 0.2619 - val_acc: 0.9439\n",
      "Epoch 11/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9825\n",
      "Epoch 00010: reducing learning rate to 8.000000525498762e-06.\n",
      "100/100 [==============================] - 47s - loss: 0.0585 - acc: 0.9825 - val_loss: 0.2614 - val_acc: 0.9443\n",
      "Epoch 12/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9844\n",
      "Epoch 00011: reducing learning rate to 1.6000001778593287e-06.\n",
      "100/100 [==============================] - 47s - loss: 0.0506 - acc: 0.9844 - val_loss: 0.2659 - val_acc: 0.9439\n",
      "Epoch 13/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9848\n",
      "Epoch 00012: reducing learning rate to 3.200000264769187e-07.\n",
      "100/100 [==============================] - 47s - loss: 0.0506 - acc: 0.9849 - val_loss: 0.2654 - val_acc: 0.9434\n",
      "Epoch 00012: early stopping\n",
      "evaluation on holdout:\n",
      "['loss', 'acc']\n",
      "[0.29166857409774094, 0.94574999999999998]\n",
      "158538/158538 [==============================] - 24s    \n",
      "\n",
      "fold: 5\n",
      "len(val_X): 2440\n",
      "models_dir: out/models/run_B/fold_5\n",
      "tensorboard_dir: /tensorboard/tf-speech-v2/B_5\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 47s - loss: 2.5137 - acc: 0.1531 - val_loss: 2.1372 - val_acc: 0.2971\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 47s - loss: 1.7440 - acc: 0.4141 - val_loss: 0.6410 - val_acc: 0.7918\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 48s - loss: 1.0396 - acc: 0.6578 - val_loss: 0.4345 - val_acc: 0.8639\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 47s - loss: 0.7362 - acc: 0.7638 - val_loss: 0.3297 - val_acc: 0.9041\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 47s - loss: 0.5487 - acc: 0.8262 - val_loss: 0.2942 - val_acc: 0.9123\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 47s - loss: 0.4292 - acc: 0.8656 - val_loss: 0.2672 - val_acc: 0.9328\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 47s - loss: 0.3263 - acc: 0.8984 - val_loss: 0.2542 - val_acc: 0.9357\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 47s - loss: 0.2558 - acc: 0.9211 - val_loss: 0.2638 - val_acc: 0.9352\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 47s - loss: 0.2193 - acc: 0.9328 - val_loss: 0.2452 - val_acc: 0.9439\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 47s - loss: 0.1658 - acc: 0.9493 - val_loss: 0.3424 - val_acc: 0.9152\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 48s - loss: 0.1541 - acc: 0.9544 - val_loss: 0.2401 - val_acc: 0.9488\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 47s - loss: 0.1247 - acc: 0.9626 - val_loss: 0.2712 - val_acc: 0.9426\n",
      "Epoch 13/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1036 - acc: 0.9684\n",
      "Epoch 00012: reducing learning rate to 0.00020000000949949026.\n",
      "100/100 [==============================] - 47s - loss: 0.1035 - acc: 0.9684 - val_loss: 0.3045 - val_acc: 0.9496\n",
      "Epoch 14/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9912\n",
      "Epoch 00013: reducing learning rate to 4.0000001899898055e-05.\n",
      "100/100 [==============================] - 47s - loss: 0.0299 - acc: 0.9913 - val_loss: 0.3661 - val_acc: 0.9545\n",
      "Epoch 15/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9957\n",
      "Epoch 00014: reducing learning rate to 8.000000525498762e-06.\n",
      "100/100 [==============================] - 47s - loss: 0.0148 - acc: 0.9957 - val_loss: 0.3591 - val_acc: 0.9529\n",
      "Epoch 16/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9965\n",
      "Epoch 00015: reducing learning rate to 1.6000001778593287e-06.\n",
      "100/100 [==============================] - 47s - loss: 0.0118 - acc: 0.9965 - val_loss: 0.3640 - val_acc: 0.9545\n",
      "Epoch 17/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9967\n",
      "Epoch 00016: reducing learning rate to 3.200000264769187e-07.\n",
      "100/100 [==============================] - 47s - loss: 0.0121 - acc: 0.9967 - val_loss: 0.3642 - val_acc: 0.9541\n",
      "Epoch 00016: early stopping\n",
      "evaluation on holdout:\n",
      "['loss', 'acc']\n",
      "[0.35491069688209698, 0.95174999999999998]\n",
      "158538/158538 [==============================] - 24s    \n",
      "\n",
      "fold: 6\n",
      "len(val_X): 2440\n",
      "models_dir: out/models/run_B/fold_6\n",
      "tensorboard_dir: /tensorboard/tf-speech-v2/B_6\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 47s - loss: 2.5906 - acc: 0.1484 - val_loss: 1.7600 - val_acc: 0.4037\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 47s - loss: 1.7514 - acc: 0.4192 - val_loss: 1.4987 - val_acc: 0.4906\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 47s - loss: 1.0741 - acc: 0.6499 - val_loss: 0.6608 - val_acc: 0.7680\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 47s - loss: 0.7737 - acc: 0.7521 - val_loss: 0.3097 - val_acc: 0.9008\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 47s - loss: 0.5721 - acc: 0.8182 - val_loss: 0.2492 - val_acc: 0.9225\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 47s - loss: 0.4542 - acc: 0.8579 - val_loss: 0.2899 - val_acc: 0.9209\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 47s - loss: 0.3481 - acc: 0.8903 - val_loss: 0.2441 - val_acc: 0.9389\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 47s - loss: 0.2771 - acc: 0.9143 - val_loss: 0.2570 - val_acc: 0.9414\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 47s - loss: 0.2242 - acc: 0.9307 - val_loss: 0.2176 - val_acc: 0.9516\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 47s - loss: 0.1749 - acc: 0.9454 - val_loss: 0.2751 - val_acc: 0.9398\n",
      "Epoch 11/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.9536\n",
      "Epoch 00010: reducing learning rate to 0.00020000000949949026.\n",
      "100/100 [==============================] - 47s - loss: 0.1534 - acc: 0.9538 - val_loss: 0.2768 - val_acc: 0.9455\n",
      "Epoch 12/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9843\n",
      "Epoch 00011: reducing learning rate to 4.0000001899898055e-05.\n",
      "100/100 [==============================] - 47s - loss: 0.0516 - acc: 0.9843 - val_loss: 0.3047 - val_acc: 0.9516\n",
      "Epoch 13/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9916\n",
      "Epoch 00012: reducing learning rate to 8.000000525498762e-06.\n",
      "100/100 [==============================] - 47s - loss: 0.0272 - acc: 0.9916 - val_loss: 0.3320 - val_acc: 0.9520\n",
      "Epoch 14/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9931\n",
      "Epoch 00013: reducing learning rate to 1.6000001778593287e-06.\n",
      "100/100 [==============================] - 47s - loss: 0.0239 - acc: 0.9932 - val_loss: 0.3332 - val_acc: 0.9520\n",
      "Epoch 15/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9928\n",
      "Epoch 00014: reducing learning rate to 3.200000264769187e-07.\n",
      "100/100 [==============================] - 47s - loss: 0.0231 - acc: 0.9928 - val_loss: 0.3332 - val_acc: 0.9520\n",
      "Epoch 00014: early stopping\n",
      "evaluation on holdout:\n",
      "['loss', 'acc']\n",
      "[0.3269985166520637, 0.95125000000000004]\n",
      "158538/158538 [==============================] - 24s    \n",
      "\n",
      "fold: 7\n",
      "len(val_X): 2440\n",
      "models_dir: out/models/run_B/fold_7\n",
      "tensorboard_dir: /tensorboard/tf-speech-v2/B_7\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 48s - loss: 2.3634 - acc: 0.1963 - val_loss: 1.4810 - val_acc: 0.4857\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 47s - loss: 1.5412 - acc: 0.4886 - val_loss: 0.6624 - val_acc: 0.7799\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 47s - loss: 0.9726 - acc: 0.6837 - val_loss: 0.4060 - val_acc: 0.8730\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 48s - loss: 0.6875 - acc: 0.7794 - val_loss: 0.3159 - val_acc: 0.9061\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 47s - loss: 0.5413 - acc: 0.8309 - val_loss: 0.2866 - val_acc: 0.9209\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 47s - loss: 0.4118 - acc: 0.8706 - val_loss: 0.2785 - val_acc: 0.9242\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 47s - loss: 0.3169 - acc: 0.9008 - val_loss: 0.2562 - val_acc: 0.9336\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 47s - loss: 0.2492 - acc: 0.9230 - val_loss: 0.3219 - val_acc: 0.9266\n",
      "Epoch 9/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2013 - acc: 0.9377\n",
      "Epoch 00008: reducing learning rate to 0.00020000000949949026.\n",
      "100/100 [==============================] - 47s - loss: 0.2012 - acc: 0.9377 - val_loss: 0.2730 - val_acc: 0.9381\n",
      "Epoch 10/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0715 - acc: 0.9785\n",
      "Epoch 00009: reducing learning rate to 4.0000001899898055e-05.\n",
      "100/100 [==============================] - 47s - loss: 0.0714 - acc: 0.9785 - val_loss: 0.3350 - val_acc: 0.9410\n",
      "Epoch 11/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9868\n",
      "Epoch 00010: reducing learning rate to 8.000000525498762e-06.\n",
      "100/100 [==============================] - 47s - loss: 0.0439 - acc: 0.9868 - val_loss: 0.3360 - val_acc: 0.9422\n",
      "Epoch 12/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9887\n",
      "Epoch 00011: reducing learning rate to 1.6000001778593287e-06.\n",
      "100/100 [==============================] - 47s - loss: 0.0378 - acc: 0.9887 - val_loss: 0.3367 - val_acc: 0.9410\n",
      "Epoch 13/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9892\n",
      "Epoch 00012: reducing learning rate to 3.200000264769187e-07.\n",
      "100/100 [==============================] - 47s - loss: 0.0358 - acc: 0.9891 - val_loss: 0.3366 - val_acc: 0.9414\n",
      "Epoch 00012: early stopping\n",
      "evaluation on holdout:\n",
      "['loss', 'acc']\n",
      "[0.31621558944578282, 0.95274999999999999]\n",
      "158538/158538 [==============================] - 24s    \n",
      "\n",
      "fold: 8\n",
      "len(val_X): 2440\n",
      "models_dir: out/models/run_B/fold_8\n",
      "tensorboard_dir: /tensorboard/tf-speech-v2/B_8\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 47s - loss: 2.5979 - acc: 0.0948 - val_loss: 2.3503 - val_acc: 0.1881\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 47s - loss: 2.2005 - acc: 0.2471 - val_loss: 1.4313 - val_acc: 0.5865\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 47s - loss: 1.3467 - acc: 0.5555 - val_loss: 0.7028 - val_acc: 0.7779\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 47s - loss: 0.8892 - acc: 0.7113 - val_loss: 0.5465 - val_acc: 0.8111\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 47s - loss: 0.6508 - acc: 0.7911 - val_loss: 0.3396 - val_acc: 0.8951\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 47s - loss: 0.5080 - acc: 0.8408 - val_loss: 0.2690 - val_acc: 0.9176\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 47s - loss: 0.3921 - acc: 0.8778 - val_loss: 0.3020 - val_acc: 0.9205\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 47s - loss: 0.3117 - acc: 0.9032 - val_loss: 0.2616 - val_acc: 0.9336\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 47s - loss: 0.2484 - acc: 0.9229 - val_loss: 0.3222 - val_acc: 0.9143\n",
      "Epoch 10/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2003 - acc: 0.9387\n",
      "Epoch 00009: reducing learning rate to 0.00020000000949949026.\n",
      "100/100 [==============================] - 47s - loss: 0.2004 - acc: 0.9387 - val_loss: 0.3114 - val_acc: 0.9283\n",
      "Epoch 11/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9767\n",
      "Epoch 00010: reducing learning rate to 4.0000001899898055e-05.\n",
      "100/100 [==============================] - 47s - loss: 0.0790 - acc: 0.9768 - val_loss: 0.3026 - val_acc: 0.9406\n",
      "Epoch 12/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9856\n",
      "Epoch 00011: reducing learning rate to 8.000000525498762e-06.\n",
      "100/100 [==============================] - 47s - loss: 0.0481 - acc: 0.9856 - val_loss: 0.3260 - val_acc: 0.9422\n",
      "Epoch 13/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9873\n",
      "Epoch 00012: reducing learning rate to 1.6000001778593287e-06.\n",
      "100/100 [==============================] - 47s - loss: 0.0427 - acc: 0.9873 - val_loss: 0.3262 - val_acc: 0.9430\n",
      "Epoch 14/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9881\n",
      "Epoch 00013: reducing learning rate to 3.200000264769187e-07.\n",
      "100/100 [==============================] - 47s - loss: 0.0400 - acc: 0.9881 - val_loss: 0.3267 - val_acc: 0.9434\n",
      "Epoch 00013: early stopping\n",
      "evaluation on holdout:\n",
      "['loss', 'acc']\n",
      "[0.30604125156054213, 0.94899999999999995]\n",
      "158538/158538 [==============================] - 24s    \n",
      "\n",
      "fold: 9\n",
      "len(val_X): 2440\n",
      "models_dir: out/models/run_B/fold_9\n",
      "tensorboard_dir: /tensorboard/tf-speech-v2/B_9\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 47s - loss: 2.5238 - acc: 0.1406 - val_loss: 1.7915 - val_acc: 0.3143\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 48s - loss: 1.7570 - acc: 0.4155 - val_loss: 0.7429 - val_acc: 0.7594\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 48s - loss: 1.1038 - acc: 0.6394 - val_loss: 0.5029 - val_acc: 0.8311\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 48s - loss: 0.7654 - acc: 0.7561 - val_loss: 0.4316 - val_acc: 0.8758\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 48s - loss: 0.5692 - acc: 0.8208 - val_loss: 0.3408 - val_acc: 0.9012\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 47s - loss: 0.4317 - acc: 0.8650 - val_loss: 0.3541 - val_acc: 0.8975\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 47s - loss: 0.3442 - acc: 0.8929 - val_loss: 0.3248 - val_acc: 0.9209\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 47s - loss: 0.2694 - acc: 0.9163 - val_loss: 0.2904 - val_acc: 0.9377\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 47s - loss: 0.2079 - acc: 0.9360 - val_loss: 0.2807 - val_acc: 0.9324\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 47s - loss: 0.1930 - acc: 0.9423 - val_loss: 0.2834 - val_acc: 0.9348\n",
      "Epoch 11/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1455 - acc: 0.9560\n",
      "Epoch 00010: reducing learning rate to 0.00020000000949949026.\n",
      "100/100 [==============================] - 47s - loss: 0.1456 - acc: 0.9560 - val_loss: 0.3191 - val_acc: 0.9316\n",
      "Epoch 12/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9861\n",
      "Epoch 00011: reducing learning rate to 4.0000001899898055e-05.\n",
      "100/100 [==============================] - 47s - loss: 0.0467 - acc: 0.9861 - val_loss: 0.3640 - val_acc: 0.9471\n",
      "Epoch 13/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9921\n",
      "Epoch 00012: reducing learning rate to 8.000000525498762e-06.\n",
      "100/100 [==============================] - 47s - loss: 0.0263 - acc: 0.9921 - val_loss: 0.3859 - val_acc: 0.9475\n",
      "Epoch 14/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9937\n",
      "Epoch 00013: reducing learning rate to 1.6000001778593287e-06.\n",
      "100/100 [==============================] - 47s - loss: 0.0209 - acc: 0.9937 - val_loss: 0.3868 - val_acc: 0.9459\n",
      "Epoch 15/100\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9936\n",
      "Epoch 00014: reducing learning rate to 3.200000264769187e-07.\n",
      "100/100 [==============================] - 47s - loss: 0.0218 - acc: 0.9936 - val_loss: 0.3869 - val_acc: 0.9463\n",
      "Epoch 00014: early stopping\n",
      "evaluation on holdout:\n",
      "['loss', 'acc']\n",
      "[0.33821652156962134, 0.94650000000000001]\n",
      "158538/158538 [==============================] - 24s    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold in range(FOLDS):\n",
    "\n",
    "    print('fold:', fold)\n",
    "\n",
    "    # read val data\n",
    "    val_X = np.load('%s/val/val_X_%d.npy' % (OUT_DIR, fold))\n",
    "    val_Y = np.load('%s/val/val_Y_%d.npy' % (OUT_DIR, fold))\n",
    "    val_files = np.load('%s/val/val_files_%d.npy' % (OUT_DIR, fold))\n",
    "    assert len(val_X) == len(val_files)\n",
    "    assert len(val_Y) == len(val_files)\n",
    "    print('len(val_X):', len(val_X))\n",
    "    val_files = set(val_files)\n",
    "\n",
    "    # create dir to store models\n",
    "    models_dir = MODELS_DIR.replace('$fold$', str(fold))\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    print('models_dir:', models_dir)\n",
    "\n",
    "    def train_generator(n_per_batch):\n",
    "        while True:\n",
    "            X, Y, files = choose_batch(n_per_batch, train_X, train_Y,\n",
    "                                       train_files, val_files)\n",
    "            yield (X, Y)\n",
    "\n",
    "    # rm/create tensorboard dir\n",
    "    tensorboard_dir = TENSORBOARD_DIR.replace('$fold$', str(fold))\n",
    "    shutil.rmtree(tensorboard_dir, ignore_errors=True)\n",
    "    os.makedirs(tensorboard_dir)\n",
    "    print('tensorboard_dir:', tensorboard_dir)\n",
    "\n",
    "    # create model\n",
    "    model = Model_3(input_size=INPUT_SIZE, output_size=len(LABELS))\n",
    "    model.build()\n",
    "    optimizer = RMSprop(lr=1e-3)\n",
    "    model.m.compile(\n",
    "        optimizer=optimizer, loss=categorical_crossentropy, metrics=['accuracy']\\\n",
    "    )\n",
    "\n",
    "    model.m.fit_generator(\n",
    "        train_generator(N_PER_BATCH),\n",
    "        STEPS_PER_EPOCH,\n",
    "        epochs=N_EPOCHS,\n",
    "        validation_data=(val_X, val_Y),\n",
    "        callbacks=[\n",
    "            TensorBoard(log_dir=tensorboard_dir),\n",
    "            ModelCheckpoint(\n",
    "                models_dir +\n",
    "                '/e{epoch:03d}-l={loss:.5f}-vl={val_loss:.5f}-a={acc:.5f}-va={val_acc:.5f}.h5',\n",
    "                monitor='val_acc',\n",
    "                verbose=0,\n",
    "                save_best_only=True,\n",
    "                save_weights_only=False,\n",
    "                mode='auto'),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.2,\n",
    "                patience=1,\n",
    "                min_lr=1e-9,\n",
    "                verbose=1),\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                min_delta=0.000001,\n",
    "                patience=5,\n",
    "                verbose=1,\n",
    "                mode='auto')\n",
    "        ])\n",
    "\n",
    "    # predict on holdout\n",
    "    holdout_X = np.load('%s/holdout/holdout_X.npy' % (OUT_DIR))\n",
    "    holdout_Y = np.load('%s/holdout/holdout_Y.npy' % (OUT_DIR))\n",
    "    hp = model.m.predict(holdout_X)\n",
    "    np.save('%s/holdout/holdout_predictions_%d.npy' % (OUT_DIR, fold), hp)\n",
    "\n",
    "    # eval on holdout\n",
    "    print('evaluation on holdout:')\n",
    "    print(model.m.metrics_names)\n",
    "    print(model.m.evaluate(holdout_X, holdout_Y, verbose=0))\n",
    "\n",
    "    # predict on test data\n",
    "    test_predictions = model.m.predict(test_X, verbose=1, batch_size=1000)\n",
    "    np.save('%s/test/test_predictions_%d.npy' % (OUT_DIR, fold),\n",
    "            test_predictions)\n",
    "\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
