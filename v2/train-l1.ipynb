{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import shutil\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from keras.optimizers import *\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = '0'\n",
    "RND = 777\n",
    "RUN = 'D'\n",
    "OUT_DIR = 'out'\n",
    "TENSORBOARD_DIR = '/tensorboard/tf-speech-v2/%s_$fold$' % RUN\n",
    "MODELS_DIR = '%s/models/run_%s/fold_$fold$' % (OUT_DIR, RUN)\n",
    "INPUT_SIZE = (96, 96, 1)  # n_mels x width x 1ch\n",
    "FOLDS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RND)\n",
    "seed(RND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make only specific GPU to be utilized\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '../data-generator.ipynb'\n",
    "%run '../models.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_batch(n, train_X, train_Y, train_files, val_files):\n",
    "    assert isinstance(val_files, set)\n",
    "\n",
    "    # extra random indexes to search for files not in val_files\n",
    "    def _extra_indexes():\n",
    "        return np.random.randint(0, len(train_X), size=int(n * 0.15))\n",
    "\n",
    "    ii = np.random.randint(0, len(train_X), size=n)\n",
    "    extra_ii = []\n",
    "\n",
    "    replaced = 0\n",
    "\n",
    "    # replace indexes with files occuring in val_files\n",
    "    for j in range(len(ii)):\n",
    "        if '(silence)' != train_files[ii[j]]:\n",
    "            while train_files[ii[j]] in val_files:\n",
    "                if len(extra_ii) == 0: extra_ii = _extra_indexes()\n",
    "                ii[j], extra_ii = extra_ii[0], extra_ii[1:]\n",
    "                replaced += 1\n",
    "\n",
    "    X = train_X[ii]\n",
    "    Y = train_Y[ii]\n",
    "    files = train_files[ii]\n",
    "    \n",
    "    return X, Y, files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_X): 1000000\n"
     ]
    }
   ],
   "source": [
    "train_X = np.memmap('%s/train_X.mem' % OUT_DIR, np.float32,\n",
    "                    'r').reshape((-1, ) + INPUT_SIZE)\n",
    "train_Y = np.memmap('%s/train_Y.mem' % OUT_DIR, np.float32, 'r').reshape(\n",
    "    (-1, len(LABELS)))\n",
    "\n",
    "train_files = np.load('%s/train_files.npy' % OUT_DIR)\n",
    "\n",
    "assert len(train_Y) == len(train_X)\n",
    "assert len(train_files) == len(train_X)\n",
    "\n",
    "print('len(train_X):', len(train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "N_PER_BATCH = 500\n",
    "# last number splits train set into XX epochs\n",
    "STEPS_PER_EPOCH = len(train_X) // N_PER_BATCH // 10\n",
    "N_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = np.memmap('%s/test/test_X.mem' % (OUT_DIR), np.float32,\n",
    "                   'r').reshape((-1, ) + INPUT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0\n",
      "len(val_X): 2441\n",
      "models_dir: out/models/run_D/fold_0\n",
      "tensorboard_dir: /tensorboard/tf-speech-v2/D_0\n",
      "Epoch 1/100\n",
      "200/200 [==============================] - 140s - loss: 1.3836 - acc: 0.5808 - val_loss: 1.5333 - val_acc: 0.6723\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 135s - loss: 0.5963 - acc: 0.8080 - val_loss: 0.4971 - val_acc: 0.9050\n",
      "Epoch 3/100\n",
      " 80/200 [===========>..................] - ETA: 80s - loss: 0.1041 - acc: 0.9707"
     ]
    }
   ],
   "source": [
    "for fold in range(0, FOLDS // 2):\n",
    "\n",
    "    print('fold:', fold)\n",
    "\n",
    "    # read val data\n",
    "    val_X = np.load('%s/val/val_X_%d.npy' % (OUT_DIR, fold))\n",
    "    val_Y = np.load('%s/val/val_Y_%d.npy' % (OUT_DIR, fold))\n",
    "    val_files = np.load('%s/val/val_files_%d.npy' % (OUT_DIR, fold))\n",
    "    assert len(val_X) == len(val_files)\n",
    "    assert len(val_Y) == len(val_files)\n",
    "    print('len(val_X):', len(val_X))\n",
    "    val_files = set(val_files)\n",
    "\n",
    "    # create dir to store models\n",
    "    models_dir = MODELS_DIR.replace('$fold$', str(fold))\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    print('models_dir:', models_dir)\n",
    "\n",
    "    def train_generator(n_per_batch):\n",
    "        while True:\n",
    "            X, Y, files = choose_batch(n_per_batch, train_X, train_Y,\n",
    "                                       train_files, val_files)\n",
    "            yield (X, Y)\n",
    "\n",
    "    # rm/create tensorboard dir\n",
    "    tensorboard_dir = TENSORBOARD_DIR.replace('$fold$', str(fold))\n",
    "    shutil.rmtree(tensorboard_dir, ignore_errors=True)\n",
    "    os.makedirs(tensorboard_dir)\n",
    "    print('tensorboard_dir:', tensorboard_dir)\n",
    "\n",
    "    # create model\n",
    "    model = Model_4(input_size=INPUT_SIZE, output_size=len(LABELS))\n",
    "    model.build()\n",
    "    optimizer = RMSprop(lr=1e-3)\n",
    "    model.m.compile(\n",
    "        optimizer=optimizer, loss=categorical_crossentropy, metrics=['accuracy']\\\n",
    "    )\n",
    "\n",
    "    model.m.fit_generator(\n",
    "        train_generator(N_PER_BATCH),\n",
    "        STEPS_PER_EPOCH,\n",
    "        epochs=N_EPOCHS,\n",
    "        validation_data=(val_X, val_Y),\n",
    "        callbacks=[\n",
    "            TensorBoard(log_dir=tensorboard_dir),\n",
    "            ModelCheckpoint(\n",
    "                models_dir +\n",
    "                '/e{epoch:03d}-l={loss:.5f}-vl={val_loss:.5f}-a={acc:.5f}-va={val_acc:.5f}.h5',\n",
    "                monitor='val_acc',\n",
    "                verbose=0,\n",
    "                save_best_only=True,\n",
    "                save_weights_only=False,\n",
    "                mode='auto'),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='loss', factor=0.2, patience=1, min_lr=1e-9,\n",
    "                verbose=1),\n",
    "            EarlyStopping(\n",
    "                monitor='val_acc',\n",
    "                min_delta=0.000001,\n",
    "                patience=5,\n",
    "                verbose=1,\n",
    "                mode='auto')\n",
    "        ])\n",
    "\n",
    "    # predict on holdout\n",
    "    holdout_X = np.load('%s/holdout/holdout_X.npy' % (OUT_DIR))\n",
    "    holdout_Y = np.load('%s/holdout/holdout_Y.npy' % (OUT_DIR))\n",
    "    hp = model.m.predict(holdout_X)\n",
    "    np.save('%s/holdout/holdout_predictions_%d.npy' % (OUT_DIR, fold), hp)\n",
    "\n",
    "    # eval on holdout\n",
    "    print('evaluation on holdout:')\n",
    "    print(model.m.metrics_names)\n",
    "    print(model.m.evaluate(holdout_X, holdout_Y, verbose=0))\n",
    "\n",
    "    # predict on test data\n",
    "    test_predictions = model.m.predict(test_X, verbose=1, batch_size=100)\n",
    "    np.save('%s/test/test_predictions_%d.npy' % (OUT_DIR, fold),\n",
    "            test_predictions)\n",
    "\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "\n",
    "# m1 = load_model('out_64x64_1m/models/run_B/fold_9/e012-l=0.02628-vl=0.38592-a=0.99213-va=0.94754.h5')\n",
    "\n",
    "# m2 = load_model('out_64x64_1m/models/run_B/fold_1/e007-l=0.19546-vl=0.30110-a=0.93992-va=0.93200.h5')\n",
    "\n",
    "# m3 = load_model('out_64x64_1m/models/run_B/fold_5/e013-l=0.02991-vl=0.36607-a=0.99127-va=0.95451.h5')\n",
    "\n",
    "# for i in range(1000):\n",
    "#     d = np.random.randn(1, 64, 64, 1)\n",
    "#     l1 = LABELS[np.argmax(m1.predict(d))]\n",
    "#     l2 = LABELS[np.argmax(m2.predict(d))]\n",
    "#     l3 = LABELS[np.argmax(m3.predict(d))]\n",
    "#     if l1 != 'silence' or l2 != 'silence' or l3 != 'silence': print(l1, l2, l3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
