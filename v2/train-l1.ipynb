{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import shutil\n",
    "import numpy as np\n",
    "from random import seed\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from keras.optimizers import *\n",
    "from keras.losses import *\n",
    "from keras.callbacks import *\n",
    "from keras.utils.training_utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPUS = '0,1'\n",
    "RND = 777\n",
    "RUN = 'E5'\n",
    "OUT_DIR = 'out'\n",
    "TENSORBOARD_DIR = '/tensorboard/tf-speech-v2/%s_$fold$' % RUN\n",
    "MODELS_DIR = '%s/models/run_%s/fold_$fold$' % (OUT_DIR, RUN)\n",
    "INPUT_SIZE = (96, 96, 1)  # n_mels x width x 1ch\n",
    "FOLDS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RND)\n",
    "seed(RND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions dir\n",
    "os.makedirs('%s/predictions/run_%s'%(OUT_DIR, RUN), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make only specific GPU to be utilized\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = GPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '../data-generator.ipynb'\n",
    "%run '../models.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_batch(n, train_X, train_Y, train_files, val_files):\n",
    "    assert isinstance(val_files, set)\n",
    "\n",
    "    # extra random indexes to search for files not in val_files\n",
    "    def _extra_indexes():\n",
    "        return np.random.randint(0, len(train_X), size=int(n * 0.15))\n",
    "\n",
    "    ii = np.random.randint(0, len(train_X), size=n)\n",
    "    extra_ii = []\n",
    "\n",
    "    replaced = 0\n",
    "\n",
    "    # replace indexes with files occuring in val_files\n",
    "    for j in range(len(ii)):\n",
    "        if '(silence)' != train_files[ii[j]]:\n",
    "            while train_files[ii[j]] in val_files:\n",
    "                if len(extra_ii) == 0: extra_ii = _extra_indexes()\n",
    "                ii[j], extra_ii = extra_ii[0], extra_ii[1:]\n",
    "                replaced += 1\n",
    "\n",
    "    X = train_X[ii]\n",
    "    Y = train_Y[ii]\n",
    "    files = train_files[ii]\n",
    "    \n",
    "    return X, Y, files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_X): 1000000\n"
     ]
    }
   ],
   "source": [
    "train_X = np.memmap('%s/train_X.mem' % OUT_DIR, np.float32,\n",
    "                    'r').reshape((-1, ) + INPUT_SIZE)\n",
    "train_Y = np.memmap('%s/train_Y.mem' % OUT_DIR, np.float32, 'r').reshape(\n",
    "    (-1, len(LABELS)))\n",
    "\n",
    "train_files = np.load('%s/train_files.npy' % OUT_DIR)\n",
    "\n",
    "assert len(train_Y) == len(train_X)\n",
    "assert len(train_files) == len(train_X)\n",
    "\n",
    "print('len(train_X):', len(train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "N_PER_BATCH = 500\n",
    "# last number splits train set into XX epochs\n",
    "STEPS_PER_EPOCH = len(train_X) // N_PER_BATCH // 10\n",
    "N_EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = np.memmap('%s/test/test_X.mem' % (OUT_DIR), np.float32,\n",
    "                   'r').reshape((-1, ) + INPUT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0\n",
      "len(val_X): 2441\n",
      "models_dir: out/models/run_E5/fold_0\n",
      "tensorboard_dir: /tensorboard/tf-speech-v2/E5_0\n",
      "Epoch 1/200\n",
      "200/200 [==============================] - 143s 713ms/step - loss: 2.0305 - acc: 0.3833 - val_loss: 1.3150 - val_acc: 0.6178\n",
      "Epoch 2/200\n",
      "200/200 [==============================] - 139s 693ms/step - loss: 1.1149 - acc: 0.6236 - val_loss: 0.4954 - val_acc: 0.8476\n",
      "Epoch 3/200\n",
      "200/200 [==============================] - 138s 691ms/step - loss: 0.8038 - acc: 0.7362 - val_loss: 0.4401 - val_acc: 0.8578\n",
      "Epoch 4/200\n",
      "200/200 [==============================] - 139s 693ms/step - loss: 0.6455 - acc: 0.7902 - val_loss: 0.3071 - val_acc: 0.9095\n",
      "Epoch 5/200\n",
      "200/200 [==============================] - 139s 697ms/step - loss: 0.5445 - acc: 0.8245 - val_loss: 0.2843 - val_acc: 0.9123\n",
      "Epoch 6/200\n",
      "200/200 [==============================] - 138s 690ms/step - loss: 0.4842 - acc: 0.8457 - val_loss: 0.4571 - val_acc: 0.8660\n",
      "Epoch 7/200\n",
      "200/200 [==============================] - 139s 697ms/step - loss: 0.4313 - acc: 0.8627 - val_loss: 0.2461 - val_acc: 0.9332\n",
      "Epoch 8/200\n",
      "200/200 [==============================] - 139s 693ms/step - loss: 0.3826 - acc: 0.8784 - val_loss: 0.2504 - val_acc: 0.9336\n",
      "Epoch 9/200\n",
      "200/200 [==============================] - 139s 693ms/step - loss: 0.3457 - acc: 0.8910 - val_loss: 0.3013 - val_acc: 0.9234\n",
      "Epoch 10/200\n",
      "200/200 [==============================] - 139s 695ms/step - loss: 0.3091 - acc: 0.9012 - val_loss: 0.5998 - val_acc: 0.8628\n",
      "Epoch 11/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.2793 - acc: 0.9111\n",
      "Epoch 00011: reducing learning rate to 1.9999999494757503e-05.\n",
      "200/200 [==============================] - 139s 695ms/step - loss: 0.2795 - acc: 0.9111 - val_loss: 0.3669 - val_acc: 0.9074\n",
      "Epoch 12/200\n",
      "200/200 [==============================] - 138s 689ms/step - loss: 0.1908 - acc: 0.9390 - val_loss: 0.1650 - val_acc: 0.9574\n",
      "Epoch 13/200\n",
      "200/200 [==============================] - 138s 689ms/step - loss: 0.1626 - acc: 0.9478 - val_loss: 0.1749 - val_acc: 0.9537\n",
      "Epoch 14/200\n",
      "200/200 [==============================] - 137s 685ms/step - loss: 0.1472 - acc: 0.9528 - val_loss: 0.1626 - val_acc: 0.9566\n",
      "Epoch 15/200\n",
      "200/200 [==============================] - 137s 687ms/step - loss: 0.1351 - acc: 0.9557 - val_loss: 0.1986 - val_acc: 0.9476\n",
      "Epoch 16/200\n",
      "200/200 [==============================] - 136s 681ms/step - loss: 0.1247 - acc: 0.9589 - val_loss: 0.2020 - val_acc: 0.9512\n",
      "Epoch 17/200\n",
      "200/200 [==============================] - 138s 689ms/step - loss: 0.1151 - acc: 0.9625 - val_loss: 0.2060 - val_acc: 0.9476\n",
      "Epoch 18/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9663\n",
      "Epoch 00018: reducing learning rate to 3.999999898951501e-06.\n",
      "200/200 [==============================] - 137s 686ms/step - loss: 0.1026 - acc: 0.9662 - val_loss: 0.2070 - val_acc: 0.9512\n",
      "Epoch 19/200\n",
      "200/200 [==============================] - 137s 686ms/step - loss: 0.0911 - acc: 0.9704 - val_loss: 0.1861 - val_acc: 0.9537\n",
      "Epoch 20/200\n",
      "200/200 [==============================] - 137s 684ms/step - loss: 0.0863 - acc: 0.9719 - val_loss: 0.1776 - val_acc: 0.9590\n",
      "Epoch 21/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9740\n",
      "Epoch 00021: reducing learning rate to 7.999999979801942e-07.\n",
      "200/200 [==============================] - 137s 687ms/step - loss: 0.0817 - acc: 0.9740 - val_loss: 0.1886 - val_acc: 0.9578\n",
      "Epoch 22/200\n",
      "200/200 [==============================] - 136s 682ms/step - loss: 0.0786 - acc: 0.9740 - val_loss: 0.1899 - val_acc: 0.9549\n",
      "Epoch 23/200\n",
      "200/200 [==============================] - 136s 681ms/step - loss: 0.0776 - acc: 0.9749 - val_loss: 0.1907 - val_acc: 0.9529\n",
      "Epoch 24/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9751\n",
      "Epoch 00024: reducing learning rate to 1.600000018697756e-07.\n",
      "200/200 [==============================] - 136s 682ms/step - loss: 0.0766 - acc: 0.9751 - val_loss: 0.1852 - val_acc: 0.9558\n",
      "Epoch 25/200\n",
      "200/200 [==============================] - 136s 682ms/step - loss: 0.0754 - acc: 0.9755 - val_loss: 0.1879 - val_acc: 0.9549\n",
      "Epoch 26/200\n",
      "200/200 [==============================] - 136s 679ms/step - loss: 0.0751 - acc: 0.9753 - val_loss: 0.1873 - val_acc: 0.9541\n",
      "Epoch 27/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9753\n",
      "Epoch 00027: reducing learning rate to 3.199999980552093e-08.\n",
      "200/200 [==============================] - 137s 687ms/step - loss: 0.0762 - acc: 0.9753 - val_loss: 0.1877 - val_acc: 0.9545\n",
      "Epoch 28/200\n",
      "200/200 [==============================] - 136s 678ms/step - loss: 0.0753 - acc: 0.9748 - val_loss: 0.1885 - val_acc: 0.9541\n",
      "Epoch 29/200\n",
      "200/200 [==============================] - 137s 684ms/step - loss: 0.0741 - acc: 0.9762 - val_loss: 0.1889 - val_acc: 0.9545\n",
      "Epoch 30/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9751\n",
      "Epoch 00030: reducing learning rate to 6.399999818995639e-09.\n",
      "200/200 [==============================] - 137s 683ms/step - loss: 0.0763 - acc: 0.9751 - val_loss: 0.1888 - val_acc: 0.9545\n",
      "Epoch 00030: early stopping\n",
      "evaluation on holdout:\n",
      "['loss', 'acc']\n",
      "[0.26040190061437896, 0.94474999999999998]\n",
      "158538/158538 [==============================] - 116s 731us/step\n",
      "\n",
      "fold: 1\n",
      "len(val_X): 2441\n",
      "models_dir: out/models/run_E5/fold_1\n",
      "tensorboard_dir: /tensorboard/tf-speech-v2/E5_1\n",
      "Epoch 1/200\n",
      "200/200 [==============================] - 138s 690ms/step - loss: 2.0064 - acc: 0.3854 - val_loss: 1.1317 - val_acc: 0.7063\n",
      "Epoch 2/200\n",
      "200/200 [==============================] - 139s 696ms/step - loss: 1.0894 - acc: 0.6316 - val_loss: 0.6091 - val_acc: 0.7931\n",
      "Epoch 3/200\n",
      "200/200 [==============================] - 138s 692ms/step - loss: 0.6428 - acc: 0.7909 - val_loss: 0.5234 - val_acc: 0.8316\n",
      "Epoch 5/200\n",
      "200/200 [==============================] - 139s 693ms/step - loss: 0.5454 - acc: 0.8253 - val_loss: 0.3572 - val_acc: 0.8996\n",
      "Epoch 6/200\n",
      "200/200 [==============================] - 138s 692ms/step - loss: 0.4811 - acc: 0.8470 - val_loss: 0.3848 - val_acc: 0.8976\n",
      "Epoch 7/200\n",
      "200/200 [==============================] - 138s 692ms/step - loss: 0.4225 - acc: 0.8650 - val_loss: 0.3845 - val_acc: 0.8869\n",
      "Epoch 8/200\n",
      "200/200 [==============================] - 138s 691ms/step - loss: 0.3788 - acc: 0.8790 - val_loss: 0.3206 - val_acc: 0.9070\n",
      "Epoch 9/200\n",
      "200/200 [==============================] - 139s 693ms/step - loss: 0.3389 - acc: 0.8929 - val_loss: 0.4247 - val_acc: 0.8775\n",
      "Epoch 10/200\n",
      "200/200 [==============================] - 137s 684ms/step - loss: 0.3122 - acc: 0.9007 - val_loss: 0.3069 - val_acc: 0.9242\n",
      "Epoch 11/200\n",
      "200/200 [==============================] - 138s 692ms/step - loss: 0.2799 - acc: 0.9114 - val_loss: 0.2681 - val_acc: 0.9291\n",
      "Epoch 12/200\n",
      "200/200 [==============================] - 139s 694ms/step - loss: 0.2589 - acc: 0.9180 - val_loss: 0.4417 - val_acc: 0.8898\n",
      "Epoch 13/200\n",
      "200/200 [==============================] - 138s 689ms/step - loss: 0.2268 - acc: 0.9276 - val_loss: 0.3006 - val_acc: 0.9267\n",
      "Epoch 14/200\n",
      "200/200 [==============================] - 138s 690ms/step - loss: 0.2077 - acc: 0.9338 - val_loss: 0.3050 - val_acc: 0.9349\n",
      "Epoch 15/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.1894 - acc: 0.9398\n",
      "Epoch 00015: reducing learning rate to 1.9999999494757503e-05.\n",
      "200/200 [==============================] - 139s 695ms/step - loss: 0.1891 - acc: 0.9398 - val_loss: 0.5901 - val_acc: 0.8755\n",
      "Epoch 16/200\n",
      "200/200 [==============================] - 138s 692ms/step - loss: 0.1080 - acc: 0.9653 - val_loss: 0.2201 - val_acc: 0.9521\n",
      "Epoch 17/200\n",
      "200/200 [==============================] - 138s 690ms/step - loss: 0.0869 - acc: 0.9720 - val_loss: 0.2603 - val_acc: 0.9512\n",
      "Epoch 18/200\n",
      "200/200 [==============================] - 138s 691ms/step - loss: 0.0799 - acc: 0.9741 - val_loss: 0.2746 - val_acc: 0.9500\n",
      "Epoch 19/200\n",
      "200/200 [==============================] - 139s 693ms/step - loss: 0.0689 - acc: 0.9779 - val_loss: 0.3700 - val_acc: 0.9418\n",
      "Epoch 20/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9792\n",
      "Epoch 00020: reducing learning rate to 3.999999898951501e-06.\n",
      "200/200 [==============================] - 139s 697ms/step - loss: 0.0650 - acc: 0.9793 - val_loss: 0.3261 - val_acc: 0.9426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/200\n",
      "200/200 [==============================] - 138s 691ms/step - loss: 0.0497 - acc: 0.9839 - val_loss: 0.2600 - val_acc: 0.9549\n",
      "Epoch 22/200\n",
      "200/200 [==============================] - 138s 691ms/step - loss: 0.0473 - acc: 0.9849 - val_loss: 0.2661 - val_acc: 0.9558\n",
      "Epoch 23/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9854\n",
      "Epoch 00023: reducing learning rate to 7.999999979801942e-07.\n",
      "200/200 [==============================] - 138s 688ms/step - loss: 0.0449 - acc: 0.9854 - val_loss: 0.2668 - val_acc: 0.9529\n",
      "Epoch 24/200\n",
      " 48/200 [======>.......................] - ETA: 1:45 - loss: 0.0425 - acc: 0.9865"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 138s 692ms/step - loss: 0.0434 - acc: 0.9861 - val_loss: 0.2741 - val_acc: 0.9537\n",
      "Epoch 29/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9863\n",
      "Epoch 00029: reducing learning rate to 3.199999980552093e-08.\n",
      "200/200 [==============================] - 138s 692ms/step - loss: 0.0436 - acc: 0.9863 - val_loss: 0.2743 - val_acc: 0.9541\n",
      "Epoch 30/200\n",
      "200/200 [==============================] - 138s 690ms/step - loss: 0.0429 - acc: 0.9863 - val_loss: 0.2745 - val_acc: 0.9541\n",
      "Epoch 31/200\n",
      "200/200 [==============================] - 138s 690ms/step - loss: 0.0425 - acc: 0.9863 - val_loss: 0.2736 - val_acc: 0.9541\n",
      "Epoch 32/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9876\n",
      "Epoch 00032: reducing learning rate to 6.399999818995639e-09.\n",
      "200/200 [==============================] - 138s 689ms/step - loss: 0.0407 - acc: 0.9875 - val_loss: 0.2729 - val_acc: 0.9541\n",
      "Epoch 00032: early stopping\n",
      "evaluation on holdout:\n",
      "['loss', 'acc']\n",
      "[0.28468323675438295, 0.94625000000000004]\n",
      "158538/158538 [==============================] - 111s 701us/step\n",
      "\n",
      "fold: 2\n",
      "len(val_X): 2441\n",
      "models_dir: out/models/run_E5/fold_2\n",
      "tensorboard_dir: /tensorboard/tf-speech-v2/E5_2\n",
      "Epoch 1/200\n",
      "200/200 [==============================] - 144s 718ms/step - loss: 1.9914 - acc: 0.3904 - val_loss: 1.0845 - val_acc: 0.7091\n",
      "Epoch 2/200\n",
      "200/200 [==============================] - 143s 715ms/step - loss: 1.0762 - acc: 0.6395 - val_loss: 0.5826 - val_acc: 0.8128\n",
      "Epoch 3/200\n",
      "200/200 [==============================] - 145s 726ms/step - loss: 0.7718 - acc: 0.7471 - val_loss: 0.4316 - val_acc: 0.8628\n",
      "Epoch 4/200\n",
      "200/200 [==============================] - 143s 716ms/step - loss: 0.6304 - acc: 0.7988 - val_loss: 0.3405 - val_acc: 0.8931\n",
      "Epoch 5/200\n",
      "200/200 [==============================] - 143s 716ms/step - loss: 0.5338 - acc: 0.8304 - val_loss: 0.2859 - val_acc: 0.9111\n",
      "Epoch 6/200\n",
      "200/200 [==============================] - 144s 719ms/step - loss: 0.4736 - acc: 0.8485 - val_loss: 0.4206 - val_acc: 0.8861\n",
      "Epoch 7/200\n",
      "200/200 [==============================] - 144s 720ms/step - loss: 0.4136 - acc: 0.8683 - val_loss: 0.4415 - val_acc: 0.8832\n",
      "Epoch 8/200\n",
      "200/200 [==============================] - 144s 718ms/step - loss: 0.3723 - acc: 0.8823 - val_loss: 0.4546 - val_acc: 0.8841\n",
      "Epoch 9/200\n",
      "200/200 [==============================] - 144s 719ms/step - loss: 0.3369 - acc: 0.8930 - val_loss: 0.2519 - val_acc: 0.9304\n",
      "Epoch 10/200\n",
      "174/200 [=========================>....] - ETA: 18s - loss: 0.3069 - acc: 0.9022"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 141s 707ms/step - loss: 1.9947 - acc: 0.3868 - val_loss: 1.0441 - val_acc: 0.6639\n",
      "Epoch 2/200\n",
      "200/200 [==============================] - 139s 696ms/step - loss: 1.0886 - acc: 0.6346 - val_loss: 0.6402 - val_acc: 0.8012\n",
      "Epoch 3/200\n",
      "200/200 [==============================] - 139s 695ms/step - loss: 0.7886 - acc: 0.7423 - val_loss: 0.3473 - val_acc: 0.8881\n",
      "Epoch 4/200\n",
      "200/200 [==============================] - 139s 695ms/step - loss: 0.6368 - acc: 0.7937 - val_loss: 0.5664 - val_acc: 0.8238\n",
      "Epoch 5/200\n",
      "200/200 [==============================] - 138s 692ms/step - loss: 0.5388 - acc: 0.8265 - val_loss: 0.5397 - val_acc: 0.8369\n",
      "Epoch 6/200\n",
      "200/200 [==============================] - 139s 697ms/step - loss: 0.4770 - acc: 0.8477 - val_loss: 0.2131 - val_acc: 0.9365\n",
      "Epoch 7/200\n",
      "200/200 [==============================] - 138s 691ms/step - loss: 0.4203 - acc: 0.8664 - val_loss: 0.3296 - val_acc: 0.9066\n",
      "Epoch 8/200\n",
      "200/200 [==============================] - 139s 694ms/step - loss: 0.3749 - acc: 0.8804 - val_loss: 0.3498 - val_acc: 0.8980\n",
      "Epoch 9/200\n",
      "200/200 [==============================] - 139s 695ms/step - loss: 0.3432 - acc: 0.8913 - val_loss: 0.2141 - val_acc: 0.9393\n",
      "Epoch 10/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.3035 - acc: 0.9021\n",
      "Epoch 00010: reducing learning rate to 1.9999999494757503e-05.\n",
      "200/200 [==============================] - 140s 698ms/step - loss: 0.3037 - acc: 0.9021 - val_loss: 0.3430 - val_acc: 0.9107\n",
      "Epoch 11/200\n",
      "200/200 [==============================] - 139s 696ms/step - loss: 0.2107 - acc: 0.9315 - val_loss: 0.2043 - val_acc: 0.9434\n",
      "Epoch 12/200\n",
      "200/200 [==============================] - 138s 691ms/step - loss: 0.1845 - acc: 0.9399 - val_loss: 0.1965 - val_acc: 0.9496\n",
      "Epoch 13/200\n",
      "200/200 [==============================] - 139s 695ms/step - loss: 0.1724 - acc: 0.9441 - val_loss: 0.2153 - val_acc: 0.9484\n",
      "Epoch 14/200\n",
      "200/200 [==============================] - 139s 693ms/step - loss: 0.1575 - acc: 0.9485 - val_loss: 0.2139 - val_acc: 0.9451\n",
      "Epoch 15/200\n",
      "200/200 [==============================] - 139s 697ms/step - loss: 0.1422 - acc: 0.9532 - val_loss: 0.2015 - val_acc: 0.9508\n",
      "Epoch 16/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.1313 - acc: 0.9565\n",
      "Epoch 00016: reducing learning rate to 3.999999898951501e-06.\n",
      "200/200 [==============================] - 139s 696ms/step - loss: 0.1314 - acc: 0.9565 - val_loss: 0.2237 - val_acc: 0.9455\n",
      "Epoch 17/200\n",
      "200/200 [==============================] - 138s 692ms/step - loss: 0.1191 - acc: 0.9613 - val_loss: 0.2043 - val_acc: 0.9512\n",
      "Epoch 18/200\n",
      "200/200 [==============================] - 140s 699ms/step - loss: 0.1136 - acc: 0.9623 - val_loss: 0.2077 - val_acc: 0.9541\n",
      "Epoch 19/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9645\n",
      "Epoch 00019: reducing learning rate to 7.999999979801942e-07.\n",
      "200/200 [==============================] - 140s 699ms/step - loss: 0.1081 - acc: 0.9645 - val_loss: 0.2112 - val_acc: 0.9480\n",
      "Epoch 20/200\n",
      "200/200 [==============================] - 139s 693ms/step - loss: 0.1085 - acc: 0.9645 - val_loss: 0.2076 - val_acc: 0.9508\n",
      "Epoch 21/200\n",
      "200/200 [==============================] - 141s 706ms/step - loss: 0.1029 - acc: 0.9664 - val_loss: 0.2130 - val_acc: 0.9512\n",
      "Epoch 22/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9660\n",
      "Epoch 00022: reducing learning rate to 1.600000018697756e-07.\n",
      "200/200 [==============================] - 140s 698ms/step - loss: 0.1036 - acc: 0.9660 - val_loss: 0.2115 - val_acc: 0.9520\n",
      "Epoch 23/200\n",
      "200/200 [==============================] - 138s 691ms/step - loss: 0.1010 - acc: 0.9665 - val_loss: 0.2108 - val_acc: 0.9512\n",
      "Epoch 24/200\n",
      "200/200 [==============================] - 140s 698ms/step - loss: 0.1059 - acc: 0.9645 - val_loss: 0.2099 - val_acc: 0.9508\n",
      "Epoch 25/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.1015 - acc: 0.9665\n",
      "Epoch 00025: reducing learning rate to 3.199999980552093e-08.\n",
      "200/200 [==============================] - 139s 696ms/step - loss: 0.1014 - acc: 0.9666 - val_loss: 0.2114 - val_acc: 0.9512\n",
      "Epoch 26/200\n",
      "200/200 [==============================] - 140s 698ms/step - loss: 0.1038 - acc: 0.9655 - val_loss: 0.2114 - val_acc: 0.9512\n",
      "Epoch 27/200\n",
      "200/200 [==============================] - 141s 704ms/step - loss: 0.1013 - acc: 0.9661 - val_loss: 0.2113 - val_acc: 0.9508\n",
      "Epoch 28/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.9652\n",
      "Epoch 00028: reducing learning rate to 6.399999818995639e-09.\n",
      "200/200 [==============================] - 140s 699ms/step - loss: 0.1052 - acc: 0.9652 - val_loss: 0.2111 - val_acc: 0.9504\n",
      "Epoch 00028: early stopping\n",
      "evaluation on holdout:\n",
      "['loss', 'acc']\n",
      "[0.2308085257386556, 0.94750000000000001]\n",
      "158538/158538 [==============================] - 112s 709us/step\n",
      "\n",
      "fold: 4\n",
      "len(val_X): 2440\n",
      "models_dir: out/models/run_E5/fold_4\n",
      "tensorboard_dir: /tensorboard/tf-speech-v2/E5_4\n",
      "Epoch 1/200\n",
      "200/200 [==============================] - 140s 699ms/step - loss: 2.0470 - acc: 0.3849 - val_loss: 1.1405 - val_acc: 0.7189\n",
      "Epoch 2/200\n",
      "110/200 [===============>..............] - ETA: 1:02 - loss: 1.2330 - acc: 0.5828"
     ]
    }
   ],
   "source": [
    "for fold in range(FOLDS):\n",
    "\n",
    "    print('fold:', fold)\n",
    "\n",
    "    # read val data\n",
    "    val_X = np.load('%s/val/val_X_%d.npy' % (OUT_DIR, fold))\n",
    "    val_Y = np.load('%s/val/val_Y_%d.npy' % (OUT_DIR, fold))\n",
    "    val_files = np.load('%s/val/val_files_%d.npy' % (OUT_DIR, fold))\n",
    "    assert len(val_X) == len(val_files)\n",
    "    assert len(val_Y) == len(val_files)\n",
    "    print('len(val_X):', len(val_X))\n",
    "    val_files = set(val_files)\n",
    "\n",
    "    # create dir to store models\n",
    "    models_dir = MODELS_DIR.replace('$fold$', str(fold))\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    print('models_dir:', models_dir)\n",
    "\n",
    "    def train_generator(n_per_batch):\n",
    "        while True:\n",
    "            X, Y, files = choose_batch(n_per_batch, train_X, train_Y,\n",
    "                                       train_files, val_files)\n",
    "            yield (X, Y)\n",
    "\n",
    "    # rm/create tensorboard dir\n",
    "    tensorboard_dir = TENSORBOARD_DIR.replace('$fold$', str(fold))\n",
    "    shutil.rmtree(tensorboard_dir, ignore_errors=True)\n",
    "    os.makedirs(tensorboard_dir)\n",
    "    print('tensorboard_dir:', tensorboard_dir)\n",
    "\n",
    "    # create model\n",
    "    model = Model_6(input_size=INPUT_SIZE, output_size=len(LABELS))\n",
    "    model.build()\n",
    "\n",
    "    # use x gpus\n",
    "    \n",
    "    n_gpus = len(GPUS.split(','))\n",
    "    \n",
    "    if n_gpus > 1:\n",
    "        m = multi_gpu_model(model.m, gpus=n_gpus)\n",
    "    else:\n",
    "        m = model.m\n",
    "\n",
    "    # fit model\n",
    "    \n",
    "    optimizer = RMSprop(lr=1e-4)\n",
    "    m.compile(\n",
    "        optimizer=optimizer, loss=categorical_crossentropy, metrics=['accuracy']\\\n",
    "    )\n",
    "\n",
    "    m.fit_generator(\n",
    "        train_generator(N_PER_BATCH),\n",
    "        STEPS_PER_EPOCH,\n",
    "        epochs=N_EPOCHS,\n",
    "        validation_data=(val_X, val_Y),\n",
    "        callbacks=[\n",
    "            TensorBoard(log_dir=tensorboard_dir),\n",
    "            #             ModelCheckpoint(\n",
    "            #                 models_dir +\n",
    "            #                 '/e{epoch:03d}-l={loss:.5f}-vl={val_loss:.5f}-a={acc:.5f}-va={val_acc:.5f}.h5',\n",
    "            #                 monitor='val_acc',\n",
    "            #                 verbose=0,\n",
    "            #                 save_best_only=True,\n",
    "            #                 save_weights_only=False,\n",
    "            #                 mode='auto'),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_loss', factor=0.2, patience=3, min_lr=1e-9,\n",
    "                verbose=1),\n",
    "            EarlyStopping(\n",
    "                monitor='val_acc',\n",
    "                min_delta=0.000001,\n",
    "                patience=10,\n",
    "                verbose=1,\n",
    "                mode='auto')\n",
    "        ])\n",
    "\n",
    "    # predict on holdout\n",
    "    holdout_X = np.load('%s/holdout/holdout_X.npy' % (OUT_DIR))\n",
    "    holdout_Y = np.load('%s/holdout/holdout_Y.npy' % (OUT_DIR))\n",
    "    hp = m.predict(holdout_X)\n",
    "    np.save('%s/predictions/run_%s/holdout_predictions_%d.npy' % (OUT_DIR, RUN,\n",
    "                                                                  fold), hp)\n",
    "\n",
    "    # eval on holdout\n",
    "    print('evaluation on holdout:')\n",
    "    print(m.metrics_names)\n",
    "    print(m.evaluate(holdout_X, holdout_Y, verbose=0))\n",
    "\n",
    "    # predict on test data\n",
    "    test_predictions = m.predict(test_X, verbose=1, batch_size=100)\n",
    "    np.save('%s/predictions/run_%s/test_predictions_%d.npy' %\n",
    "            (OUT_DIR, RUN, fold), test_predictions)\n",
    "\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .4031 .7079 .8292"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
