{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import shutil\n",
    "import numpy as np\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '../lib.ipynb'\n",
    "%run '../data-generator.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make only specific GPU to be utilized\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOLDOUT_PREDS = 'out/holdout/holdout_predictions_%d.npy'\n",
    "HOLDOUT_Y = 'out/holdout/holdout_Y.npy'\n",
    "FOLDS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_preds.shape: (4000, 10, 12)\n"
     ]
    }
   ],
   "source": [
    "# read holdout predictions\n",
    "h_preds = np.zeros((FOLDS, ) + np.load(HOLDOUT_PREDS % 0).shape)\n",
    "\n",
    "for fold in range(FOLDS):\n",
    "    h_preds[fold] = np.load(HOLDOUT_PREDS % fold)\n",
    "\n",
    "h_preds = np.swapaxes(h_preds, 0, 1)\n",
    "print('h_preds.shape:', h_preds.shape) # i, fold, label_1h\n",
    "\n",
    "# read holdout Ys\n",
    "holdout_Y = np.load(HOLDOUT_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10, 12)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               61952     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 12)                780       \n",
      "=================================================================\n",
      "Total params: 235,212\n",
      "Trainable params: 235,212\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "\n",
    "i = Input(shape=(h_preds.shape[1:]))\n",
    "x = Flatten()(i)\n",
    "x = Dense(512, activation='sigmoid')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='sigmoid')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation='sigmoid')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64, activation='sigmoid')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(h_preds.shape[-1], activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=[i], outputs=[x])\n",
    "model.compile(\n",
    "    optimizer=RMSprop(lr=1e-3),\n",
    "    loss=categorical_crossentropy,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4000/4000 [==============================] - 0s - loss: 2.5937 - acc: 0.0860     \n",
      "Epoch 2/1000\n",
      "4000/4000 [==============================] - 0s - loss: 2.2374 - acc: 0.2303     \n",
      "Epoch 3/1000\n",
      "4000/4000 [==============================] - 0s - loss: 1.4469 - acc: 0.5647     \n",
      "Epoch 4/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.8884 - acc: 0.8015     \n",
      "Epoch 5/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.6147 - acc: 0.8930     \n",
      "Epoch 6/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.4758 - acc: 0.9220     \n",
      "Epoch 7/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.3958 - acc: 0.9375     \n",
      "Epoch 8/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.3523 - acc: 0.9435     \n",
      "Epoch 9/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.3389 - acc: 0.9463     \n",
      "Epoch 10/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.3163 - acc: 0.9485     \n",
      "Epoch 11/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.3133 - acc: 0.9515     \n",
      "Epoch 12/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.3098 - acc: 0.9495     \n",
      "Epoch 13/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2921 - acc: 0.9575     \n",
      "Epoch 14/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2861 - acc: 0.9547     \n",
      "Epoch 15/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.3041 - acc: 0.9567     \n",
      "Epoch 16/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2898 - acc: 0.9580     \n",
      "Epoch 17/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2851 - acc: 0.9573     \n",
      "Epoch 18/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2811 - acc: 0.9565     \n",
      "Epoch 19/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2733 - acc: 0.9592     \n",
      "Epoch 20/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2875 - acc: 0.9580     \n",
      "Epoch 21/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2680 - acc: 0.9613     \n",
      "Epoch 22/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2876 - acc: 0.9580     \n",
      "Epoch 23/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2835 - acc: 0.9595     \n",
      "Epoch 24/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2627 - acc: 0.9590     \n",
      "Epoch 25/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2768 - acc: 0.9600     \n",
      "Epoch 26/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2804 - acc: 0.9590     \n",
      "Epoch 27/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2560 - acc: 0.9587     \n",
      "Epoch 28/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2698 - acc: 0.9600     \n",
      "Epoch 29/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2670 - acc: 0.9587     \n",
      "Epoch 30/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2717 - acc: 0.9595     \n",
      "Epoch 31/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2491 - acc: 0.9607     \n",
      "Epoch 32/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2634 - acc: 0.9610     \n",
      "Epoch 33/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2577 - acc: 0.9637     \n",
      "Epoch 34/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2567 - acc: 0.9590     \n",
      "Epoch 35/1000\n",
      "3584/4000 [=========================>....] - ETA: 0s - loss: 0.2623 - acc: 0.9623\n",
      "Epoch 00034: reducing learning rate to 0.00020000000949949026.\n",
      "4000/4000 [==============================] - 0s - loss: 0.2641 - acc: 0.9617     \n",
      "Epoch 36/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2594 - acc: 0.9600     \n",
      "Epoch 37/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2504 - acc: 0.9603     \n",
      "Epoch 38/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2462 - acc: 0.9605     \n",
      "Epoch 39/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2487 - acc: 0.9633     \n",
      "Epoch 40/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2498 - acc: 0.9600     \n",
      "Epoch 41/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2456 - acc: 0.9647     \n",
      "Epoch 42/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2592 - acc: 0.9607     \n",
      "Epoch 43/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2558 - acc: 0.9620     \n",
      "Epoch 44/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2547 - acc: 0.9590     \n",
      "Epoch 45/1000\n",
      "3616/4000 [==========================>...] - ETA: 0s - loss: 0.2511 - acc: 0.9640\n",
      "Epoch 00044: reducing learning rate to 4.0000001899898055e-05.\n",
      "4000/4000 [==============================] - 0s - loss: 0.2550 - acc: 0.9640     \n",
      "Epoch 46/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2549 - acc: 0.9605     \n",
      "Epoch 47/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2439 - acc: 0.9607     \n",
      "Epoch 48/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2423 - acc: 0.9630     \n",
      "Epoch 49/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2436 - acc: 0.9620     \n",
      "Epoch 50/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2393 - acc: 0.9615     \n",
      "Epoch 51/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2453 - acc: 0.9613     \n",
      "Epoch 52/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2348 - acc: 0.9620     \n",
      "Epoch 53/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2399 - acc: 0.9603     \n",
      "Epoch 54/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2539 - acc: 0.9630     \n",
      "Epoch 55/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2431 - acc: 0.9630     \n",
      "Epoch 56/1000\n",
      "3552/4000 [=========================>....] - ETA: 0s - loss: 0.2719 - acc: 0.9583\n",
      "Epoch 00055: reducing learning rate to 8.000000525498762e-06.\n",
      "4000/4000 [==============================] - 0s - loss: 0.2681 - acc: 0.9592     \n",
      "Epoch 57/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2526 - acc: 0.9620     \n",
      "Epoch 58/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2441 - acc: 0.9620     \n",
      "Epoch 59/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2322 - acc: 0.9613     \n",
      "Epoch 60/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2445 - acc: 0.9617     \n",
      "Epoch 61/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2518 - acc: 0.9613     \n",
      "Epoch 62/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2533 - acc: 0.9605     \n",
      "Epoch 63/1000\n",
      "3584/4000 [=========================>....] - ETA: 0s - loss: 0.2389 - acc: 0.9629\n",
      "Epoch 00062: reducing learning rate to 1.6000001778593287e-06.\n",
      "4000/4000 [==============================] - 0s - loss: 0.2438 - acc: 0.9620     \n",
      "Epoch 64/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2458 - acc: 0.9600     \n",
      "Epoch 65/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2498 - acc: 0.9615     \n",
      "Epoch 66/1000\n",
      "3616/4000 [==========================>...] - ETA: 0s - loss: 0.2354 - acc: 0.9610\n",
      "Epoch 00065: reducing learning rate to 3.200000264769187e-07.\n",
      "4000/4000 [==============================] - 0s - loss: 0.2360 - acc: 0.9605     \n",
      "Epoch 67/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2331 - acc: 0.9645     \n",
      "Epoch 68/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2354 - acc: 0.9617     \n",
      "Epoch 69/1000\n",
      "3488/4000 [=========================>....] - ETA: 0s - loss: 0.2432 - acc: 0.9610\n",
      "Epoch 00068: reducing learning rate to 6.400000529538374e-08.\n",
      "4000/4000 [==============================] - 0s - loss: 0.2439 - acc: 0.9605     \n",
      "Epoch 70/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2450 - acc: 0.9625     \n",
      "Epoch 71/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2471 - acc: 0.9633     \n",
      "Epoch 72/1000\n",
      "3552/4000 [=========================>....] - ETA: 0s - loss: 0.2462 - acc: 0.9611\n",
      "Epoch 00071: reducing learning rate to 1.2800001059076749e-08.\n",
      "4000/4000 [==============================] - 0s - loss: 0.2473 - acc: 0.9610     \n",
      "Epoch 73/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2477 - acc: 0.9615     \n",
      "Epoch 74/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2361 - acc: 0.9630     \n",
      "Epoch 75/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3552/4000 [=========================>....] - ETA: 0s - loss: 0.2704 - acc: 0.9583\n",
      "Epoch 00074: reducing learning rate to 2.5600002118153498e-09.\n",
      "4000/4000 [==============================] - 0s - loss: 0.2565 - acc: 0.9600     \n",
      "Epoch 76/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2484 - acc: 0.9627     \n",
      "Epoch 77/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2445 - acc: 0.9622     \n",
      "Epoch 78/1000\n",
      "3584/4000 [=========================>....] - ETA: 0s - loss: 0.2382 - acc: 0.9615\n",
      "Epoch 00077: reducing learning rate to 5.1200004236307e-10.\n",
      "4000/4000 [==============================] - 0s - loss: 0.2431 - acc: 0.9615     \n",
      "Epoch 79/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2373 - acc: 0.9660     \n",
      "Epoch 80/1000\n",
      "4000/4000 [==============================] - 0s - loss: 0.2417 - acc: 0.9607     \n",
      "Epoch 00079: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4f5e321320>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=h_preds,\n",
    "    y=holdout_Y,\n",
    "    epochs=1000,\n",
    "    callbacks=[\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='loss', factor=0.2, patience=3, min_lr=1e-11, verbose=1),\n",
    "        EarlyStopping(\n",
    "            monitor='loss',\n",
    "            min_delta=0.0000001,\n",
    "            patience=20,\n",
    "            verbose=1,\n",
    "            mode='auto')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_Y_FILE = 'out/test/test_predictions_%d.npy'\n",
    "FNAMES_FILE = 'out/test/test_files.npy'\n",
    "OUT_FILE = 'out/submission-v2-L2-B.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read predictions\n",
    "preds_shape = np.load(TEST_Y_FILE % 0).shape\n",
    "preds = np.zeros((FOLDS, ) + preds_shape)\n",
    "\n",
    "for fold in range(FOLDS):\n",
    "    preds[fold] = np.load(TEST_Y_FILE % fold)\n",
    "\n",
    "preds = np.swapaxes(preds, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157984/158538 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "test_preds = model.predict(preds, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = np.load(FNAMES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [''] * len(fnames)\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    labels[i]= LABELS[np.argmax(test_preds[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict({'fname': fnames, 'label': labels})\n",
    "df = df.set_index('fname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(OUT_FILE, index='fname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
